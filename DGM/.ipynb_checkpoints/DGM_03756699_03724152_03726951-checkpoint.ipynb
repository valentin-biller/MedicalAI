{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hP1CF5M-O-S"
   },
   "source": [
    "# Deep Generative Models - Practical for AI in Medicine II\n",
    "\n",
    "\n",
    "\n",
    "* Bhat Maitri: 03756699\n",
    "* Biller Valentin: 03724152\n",
    "* Szabo Daniel: 03726951\n",
    "\n",
    "\n",
    "\n",
    "Date: 04.06.2024\n",
    "\n",
    "Submission deadline: *18.06.2024*\n",
    "\n",
    "For questions please contact: cosmin.bercea@tum.de\n",
    "\n",
    "Please go through the notebook to:\n",
    "\n",
    "i). complete the tasks marked with **T** (n=4) and\n",
    "\n",
    "ii). answer the questions marked with **Q** (n=5).\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this practical, you will explore the use of generative models for analyzing brain MRI scans.\n",
    "Generative models can learn the underlying probability distribution of brain MRI data and generate new synthetic images that could plausibly represent real scans.\n",
    "This has applications in generating training data, modeling disease progression, and detecting anomalies.\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "You will be working with the following datasets:\n",
    "\n",
    "- Brain MRI scans of normal subjects from the IXI dataset\n",
    "- Brain MRI scans of stroke patients with lesions from the ATLAS dataset\n",
    "\n",
    "The data consists of mid-axial slice images from the MRI volumes.\n",
    "\n",
    "\n",
    "## Tasks\n",
    "\n",
    "- **Section 2.** VAEs (30 points)\n",
    "- **Section 3.** GANs (30 points)\n",
    "- **Section 4.** Diffusion Models (40 points)\n",
    "- **Bonus.** Vision-Language Generative Models (10 points)\n",
    "\n",
    "## Tools\n",
    "\n",
    "You will use the following tools/libraries:\n",
    "\n",
    "- PyTorch\n",
    "- models/vae.py #Variational Autoencoders (VAEs)\n",
    "- models/gans.py #Generative Adversarial Networks (GANs)\n",
    "- models/diffusion.py #Diffusion Models (using a library like PyTorch Diffusion)\n",
    "- Data loaders for loading and preprocessing the MRI data\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "1. **A report (pdf)** describing your approaches, results, and findings for each of the three tasks, including sample outputs like generated images, data samples, visualizations etc.\n",
    "2. **Code** for training your generative models and applying them to the given tasks.\n",
    "\n",
    "\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCZ6db8UF-0k"
   },
   "outputs": [],
   "source": [
    "!rm -rf *\n",
    "# Clone the repository\n",
    "!git clone https://github.com/compai-lab/aim_generative_practical_24.git\n",
    "# Move all content to the current directory\n",
    "!mv ./aim_generative_practical_24/* ./\n",
    "# Remove the empty directory\n",
    "!rm -rf aim_generative_practical_24/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hii_X3izpHiU"
   },
   "outputs": [],
   "source": [
    "## Please uncomment this line only when running on Google Colab\n",
    "!pip install pytorch_lightning --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kqcMaSnfxsq"
   },
   "outputs": [],
   "source": [
    "# Install packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "from data_loader import TrainDataModule, get_all_test_dataloaders\n",
    "\n",
    "# autoreload imported modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qCL70ETfxst"
   },
   "outputs": [],
   "source": [
    "with open('./configs/train_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Reproducibility\n",
    "pl.seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXFSOeUZuHMm"
   },
   "source": [
    "## 1. Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7atOCsaxpHiV"
   },
   "outputs": [],
   "source": [
    "# # Download the data\n",
    "!wget https://syncandshare.lrz.de/dl/fiX9T1xE1eV9kKDHJ4jMUu/brain_data.zip -P ./\n",
    "# # Extract the data\n",
    "!unzip -q ./brain_data.zip -d ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhrZD23Yfxsu"
   },
   "source": [
    "### 1.1. Load and visualize the *healthy* data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15Pce4yjp-HA"
   },
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoqBwZGXfxsv"
   },
   "outputs": [],
   "source": [
    "train_data_module_healthy = TrainDataModule(\n",
    "    data_dir=config['train_data_healthy'],\n",
    "    target_size=config['target_size'],\n",
    "    batch_size=config['batch_size'])\n",
    "\n",
    "# Plot some images\n",
    "batch = next(iter(train_data_module_healthy.train_dataloader()))\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(f\"Batch min: {batch.min()}\")\n",
    "print(f\"Batch max: {batch.max()}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(batch[i].squeeze(), cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8W-RH7MG0YKr"
   },
   "source": [
    "### 1.2. Load and visualize the *pathology* data (Stroke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ep170XvN0duy"
   },
   "outputs": [],
   "source": [
    "train_data_module_stroke = TrainDataModule(\n",
    "    data_dir=config['train_data_stroke'],\n",
    "    target_size=config['target_size'],\n",
    "    batch_size=config['batch_size'])\n",
    "\n",
    "# Plot some images\n",
    "batch = next(iter(train_data_module_stroke.train_dataloader()))\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(f\"Batch min: {batch.min()}\")\n",
    "print(f\"Batch max: {batch.max()}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(batch[i].squeeze(), cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GS97PUSLpSVv"
   },
   "source": [
    "## 2. Understanding VAEs\n",
    "**Variational Autoencoders (VAEs)** consist of:\n",
    "- An **encoder** that maps input images to a latent space.\n",
    "- A **decoder** that reconstructs images from the latent space.\n",
    "\n",
    "They are trained to minimize:\n",
    "- The **reconstruction error**.\n",
    "- The **Kullback-Leibler (KL) divergence** between the learned latent distribution and a prior distribution.\n",
    "\n",
    "#### Why VAEs?\n",
    "VAEs are useful for:\n",
    "- **Image synthesis**: Generating new images similar to the training data.\n",
    "- **Anomaly detection**: Identifying unusual patterns by reconstructing normal images.\n",
    "\n",
    "#### Key Concepts\n",
    "- **Latent space**: A compressed representation of input data.\n",
    "- **KL divergence**: A measure of how one probability distribution differs from a second, expected probability distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jM7pvU6GB47e"
   },
   "source": [
    "### [10 points] ----------------------------------------- T1 (VAE) -----------------------------------------\n",
    "Implement the missing lines marked by `[TODO]` in **model/vae.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biC20jHBpHid"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5iRC-CV29aD"
   },
   "outputs": [],
   "source": [
    "from model.vae import VAELightning\n",
    "# Initialize the VAE model\n",
    "input_dim = config['target_size']  # Example input dimensions\n",
    "latent_dim = config['latent_dim']\n",
    "num_epochs = config['num_epochs']\n",
    "\n",
    "vae_model_healthy = VAELightning([1,input_dim[0],input_dim[1]], latent_dim)\n",
    "vae_model_stroke = VAELightning([1,input_dim[0],input_dim[1]], latent_dim)\n",
    "\n",
    "# Set up the trainer\n",
    "trainer_healthy = pl.Trainer(max_epochs=num_epochs)  # Adjust trainer parameters as needed\n",
    "trainer_stroke = pl.Trainer(max_epochs=num_epochs)  # Adjust trainer parameters as needed\n",
    "\n",
    "# Start training\n",
    "trainer_healthy.fit(vae_model_healthy, train_data_module_healthy.train_dataloader(), train_data_module_healthy.val_dataloader())\n",
    "trainer_stroke.fit(vae_model_stroke, train_data_module_stroke.train_dataloader(), train_data_module_stroke.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffsiDOoQ5zcR"
   },
   "outputs": [],
   "source": [
    "# Generate random samples\n",
    "vae_model_healthy.eval()\n",
    "vae_model_stroke.eval()\n",
    "with torch.no_grad():\n",
    "    random_healthy_samples = vae_model_healthy.decode(torch.randn(16, latent_dim)).cpu().numpy()\n",
    "    random_stroke_samples = vae_model_stroke.decode(torch.randn(16, latent_dim)).cpu().numpy()\n",
    "\n",
    "# Plot the random samples\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "plt.title('Healthy')\n",
    "for i in range(5):\n",
    "    ax[i].imshow(random_healthy_samples[i].squeeze(), cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "plt.show()\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "plt.title('Pathological')\n",
    "for i in range(5):\n",
    "    ax[i].imshow(random_stroke_samples[i].squeeze(), cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGaqOi0JCYK4"
   },
   "source": [
    "### [10 points] ----------------------------------------- Q1 (VAE) -----------------------------------------\n",
    "Describe the quality of the generated images from the VAE model. Are there any noticeable differences between the healthy and pathological (stroke lesion) cases? Provide possible reasons for the observed similarities or differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37t9ZH3kCnoN"
   },
   "source": [
    "### [10 points] ----------------------------------------- Q2 (VAE) -----------------------------------------\n",
    "Suggest potential ways to improve the quality and diversity of the generated images, particularly for the pathological cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HB-TgFLpVln9"
   },
   "source": [
    "## 3. Understanding GANs\n",
    "\n",
    "*Generative Adversarial Networks (GANs)* consist of:\n",
    "\n",
    "- **A Generator:** This network generates new data instances that resemble the training data.\n",
    "- **A Discriminator:** This network evaluates the authenticity of the generated data, distinguishing between real and fake data.\n",
    "\n",
    "They are trained to minimize:\n",
    "- The Generator's loss: Encourages the generator to produce data that the *discriminator cannot distinguish* from real data.\n",
    "- The Discriminator's loss: Encourages the discriminator to accurately *classify real and fake* data.\n",
    "\n",
    "**Why GANs?**\n",
    "GANs are useful for:\n",
    "- **Image synthesis:** Creating realistic images that are indistinguishable from real images.\n",
    "- **Data augmentation:** Generating additional training data to improve the performance of machine learning models.\n",
    "- **Style transfer:** Applying the style of one image to the content of another.\n",
    "\n",
    "**Key Concepts**\n",
    "- **Adversarial training:** A training process where the generator and discriminator compete against each other, improving their performance iteratively.\n",
    "- **Latent space:** A compressed representation from which the generator creates new data instances.\n",
    "- **Mode collapse:** A common issue where the generator produces limited varieties of outputs, failing to capture the diversity of the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6qHQ_bLBbit"
   },
   "outputs": [],
   "source": [
    "from model.wgan import GANModule\n",
    "# Initialize the GAN model\n",
    "hparams = {\n",
    "    'image_size': input_dim[1],\n",
    "    'latent_dim': latent_dim,\n",
    "    'channels': 1,\n",
    "    'lr': 0.0002,\n",
    "    'n_critic': 1\n",
    "}\n",
    "gan_model = GANModule(hparams)\n",
    "\n",
    "# Set up the trainer\n",
    "trainer = pl.Trainer(max_epochs=num_epochs)  # Adjust trainer parameters as needed\n",
    "\n",
    "# Start training\n",
    "trainer.fit(gan_model, train_data_module_healthy.train_dataloader(), train_data_module_healthy.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zz5jp9VURR0B"
   },
   "outputs": [],
   "source": [
    "# Generate random samples\n",
    "gan_model.eval()\n",
    "with torch.no_grad():\n",
    "    random_samples = gan_model(torch.randn(16, latent_dim)).cpu().numpy()\n",
    "\n",
    "# Plot the random samples\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "fig.suptitle('Random GAN Samples')\n",
    "for i in range(5):\n",
    "    sample_img = random_samples[i]\n",
    "    ax[i].imshow(np.squeeze(sample_img), cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkwg_2Z7TZxI"
   },
   "source": [
    "### [10 points] ----------------------------------------- Q3 (GAN) -----------------------------------------\n",
    "Describe the quality of the generated brain MRI images from the GAN model. How do they compare to the real images in terms of realism and diversity? How do they compare to VAEs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VH6uoFgBTs0D"
   },
   "source": [
    "### [5 points] ----------------------------------------- Q4 (GAN) -----------------------------------------\n",
    " How can me measure the quality of the reconstruction for GANs? Implement a metric and compare the similarity to the pathological and healthy test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwJfgz76u1fO"
   },
   "source": [
    "### [15 points] ----------------------------------------- T2 (GAN) -----------------------------------------\n",
    "Implement a metric and compare the similarity to the pathological and healthy test sets. (Hint: The metric should evaluate the closeness of two distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q50Io8TFV3o1"
   },
   "source": [
    "## 4. Understanding Diffusion Models\n",
    "\n",
    "**Why Diffusion Models?**\n",
    "Diffusion Models are useful for:\n",
    "- **Image synthesis:** Generating new images that resemble the training data.\n",
    "- **Image denoising:** Removing noise from images to restore their original quality.\n",
    "- **Inpainting and super-resolution:** Filling in missing parts of images and enhancing image resolution.\n",
    "- **Anomaly Detection:** Generating pseudo-normal samples by *partly* noising and denoising pathological images.\n",
    "\n",
    "**Key Concepts**\n",
    "\n",
    "- **Forward and Reverse Processes:** The forward process adds noise to the data, while the reverse process removes it.\n",
    "- **Markov chain:** A sequence of steps where each step depends only on the previous one, used in both the forward and reverse processes.\n",
    "- **Noise schedule:** A predefined sequence of noise levels added during the forward process, which the model learns to reverse.\n",
    "\n",
    "Diffusion models are powerful generative models that have shown significant promise in various applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pbYVVmmfxsw"
   },
   "source": [
    "**Get trained model** (DDPM with Gaussian noise trained for 1500 epochs on healthy samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBm9mrO2pHin"
   },
   "outputs": [],
   "source": [
    "!wget https://syncandshare.lrz.de/dl/fiGC15qSsdj1Tx7uojZvAT/latest_model.pt.zip # weights\n",
    "!unzip -q latest_model.pt.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mc2TKVsMpHin"
   },
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geHTxZyRpHio"
   },
   "outputs": [],
   "source": [
    "with open('./configs/diffusion_config.yaml', 'r') as f:\n",
    "    diff_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPMxoPndpHip"
   },
   "outputs": [],
   "source": [
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sp2M9dD4_DKw"
   },
   "outputs": [],
   "source": [
    "from model.ddpm import DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmh4Xt-zpHiq"
   },
   "outputs": [],
   "source": [
    "diff_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IpBNiaLpHir"
   },
   "outputs": [],
   "source": [
    "#diff_config\n",
    "anoddpm = DDPM(**diff_config)\n",
    "anoddpm.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3nDEXoyAc2f"
   },
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMDcLnr8pHis"
   },
   "outputs": [],
   "source": [
    "# load weights\n",
    "checkpoint = torch.load('latest_model.pt',map_location='cuda:0', weights_only=True)['model_weights']\n",
    "anoddpm.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnQOpjPspHit"
   },
   "outputs": [],
   "source": [
    "t=200\n",
    "x_, _ = anoddpm.sample_from_image(batch.to('cuda:0'), noise_level=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqxB7Oq1D_CP"
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "fig.suptitle('Random Diffusion Samples (t=200)')\n",
    "plt.title('Inputs')\n",
    "for i in range(5):\n",
    "    sample_img = batch[i]\n",
    "    ax[i].imshow(sample_img[0], cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Recons\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "fig.suptitle('Random Diffusion Samples (t=200)')\n",
    "plt.title('Reconstructions')\n",
    "for i in range(5):\n",
    "    sample_img = x_[i].cpu().numpy()\n",
    "    ax[i].imshow(sample_img[0], cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Anomaly maps\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "fig.suptitle('Random Diffusion Samples (t=200)')\n",
    "plt.title('Anomaly Maps')\n",
    "for i in range(5):\n",
    "    sample_img = np.abs(batch[i] - x_[i].cpu().numpy())\n",
    "    ax[i].imshow(sample_img[0], cmap='inferno')\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jbv85kk6vJZj"
   },
   "source": [
    "### [10 points] ----------------------------------------- Q5 (Diffusion) -----------------------------------------\n",
    "How does the anomaly detection performance vary with varying noise levels t? (Hint: Visually analyse the performance for different noise levels **t** (one batch is enough))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v65DBXtPvf_9"
   },
   "source": [
    "### [10 points] ----------------------------------------- T3 (Diffusion) -----------------------------------------\n",
    "Write a dataloader to load both images and ground truth masks in atlas/train/:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpzDUjerWpPk"
   },
   "source": [
    "### [20 points] ----------------------------------------- T4 (Diffusion) -----------------------------------------\n",
    "Evaluate the diffusion models on the atlas/train dataset on pixel-wise localization. Implement therefore the Dice Score and report the mean and std for the dataset. You would therefore need to select a noise level **t**. (motivate selection based on Q5).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jys17CFcxOeR"
   },
   "source": [
    "## 5. Bonus: Vision-Language Generative Models (VLMs)\n",
    "\n",
    "VLMs consist of:\n",
    "- **Image Encoder:** Transforms images into a latent representation.\n",
    "- **Text Encoder:** Converts text descriptions into a corresponding latent representation.\n",
    "- **Multimodal Fusion Module:** Aligns and integrates the visual and textual representations.\n",
    "- **Text Decoder:** Generates text outputs based on the fused multimodal representation.\n",
    "\n",
    "They are trained to minimize:\n",
    "- **Cross-Modal Matching Loss:** Ensures that the visual and textual representations are aligned in the latent space.\n",
    "- **Generation Loss:** Measures the accuracy of the generated text or image against the ground truth.\n",
    "\n",
    "**Why Vision-Language Generative Models?**\n",
    "\n",
    "*Applications and Challenges*\n",
    "Vision-Language Generative Models have numerous applications, including creating visual content from text prompts, enhancing human-computer interaction through visual assistants, and improving accessibility with automatic image descriptions. However, challenges such as understanding spatial relationships, handling high-dimensional visual data, and ensuring the reliability of generated content remain active areas of research.\n",
    "\n",
    "### [10 points] ----------------------------------------- Q6 (VLMs) -----------------------------------------\n",
    "**Outline** how Vision-Language Generative Models can be used to generate samples. Consider the following aspects:\n",
    "\n",
    "- *Training Process:* Describe the steps involved in training a VLG model, including data preparation, model architecture, and loss functions.\n",
    "- *Generation Process:* Explain how the model generates new samples, whether text from images or images from text.\n",
    "- *Applications:* Discuss potential applications of VLG models in real-world scenarios, such as automated content creation, visual question answering, and accessibility enhancements.\n",
    "- *Challenges:* Identify and elaborate on the key challenges faced in developing and deploying VLG models, such as handling high-dimensional data and ensuring the accuracy and relevance of generated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVLbMSIEXIJ0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
