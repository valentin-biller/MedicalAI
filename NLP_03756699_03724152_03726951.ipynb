{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1g8x2r2zKvDgL_YO7WsixKwCq80xoFod3","timestamp":1715611942175},{"file_id":"1YVhdEh9KDSoaPWyq8WU0b4tKr5XfTkKp","timestamp":1683023138687},{"file_id":"1I8W6SJAHopCX3Ziq5DNZJxpYTImENO7r","timestamp":1682088432737}],"collapsed_sections":["dz_Is5Bw_PZC","I62jANWNvkzp","fBPyy6E5wpVC"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"zJ9sen2-10wB"},"source":["# Hand-in Group\n","TODO: State the names of all group members and TUM-IDs"]},{"cell_type":"markdown","metadata":{"id":"dz_Is5Bw_PZC"},"source":["# Preparation"]},{"cell_type":"markdown","metadata":{"id":"3ALExTEN17Jv"},"source":["## Install Dependencies"]},{"cell_type":"code","metadata":{"id":"3ZN2hZNGvkBd"},"source":["!pip install datasets\n","!pip install tokenizers\n","!pip install transformers\n","!pip install stanza\n","# -- Initialize Stanza --\n","import stanza\n","stanza.download('en')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UGa7gWpX2ArF"},"source":["## Download Datasets\n","Note: In case the following script fails because the medical transcriptions dataset is no longer available it can be downloaded from https://www.kaggle.com/tboyle10/medicaltranscriptions ."]},{"cell_type":"code","metadata":{"id":"UGhgMCGa2C7F"},"source":["# -- OPEN-I dataset --\n","!mkdir -p open_i\n","!wget -q -N -P open_i https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz\n","!tar zxf open_i/NLMCXR_reports.tgz -C open_i\n","import glob\n","num_files = len(glob.glob(\"open_i/ecgen-radiology/*.xml\"))\n","print(f'Downloaded {num_files} files from the OpenI dataset into \"open_i/ecgen-radiology\"')\n","\n","# -- Transcriptions Dataset --\n","!mkdir -p medical_transcriptions\n","!gdown --id 1E0hm3r9bwK8cujyIcOjp_y-ZEPt1HBjn -O medical_transcriptions/medical_transcriptions.zip\n","!unzip -o medical_transcriptions/medical_transcriptions.zip -d medical_transcriptions\n","print(f'Downloaded medical transriptions dataset')\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"lbcFrCre_K84"}},{"cell_type":"markdown","metadata":{"id":"I62jANWNvkzp"},"source":["# Task 1: Pre-Processing and Vocabulary Construction (not graded!)"]},{"cell_type":"markdown","metadata":{"id":"K7Oy5fnA1iwl"},"source":["## Note\n","- Task 1 is not graded, i.e. it is not mandatory to solve this task. It insteads serves soleley to show how pre-processing and tokenization can be done.\n","- Note that tasks 2 and 3 are mandatory!\n","\n","## Goals\n","- Understand tools for text pre-processing\n","- Apply text normalization and sentence splitting to a medical corpus\n","- Train a tokenizer on a medical corpus\n","\n","## Tools\n","- [Huggingface Datasets](https://huggingface.co/docs/datasets/): Library for efficient loading, saving and processing of text datasets\n","  - Note: In many cases using pandas is sufficient for text datasets (as they often fit into memory), but Hugginface Datasets provides some usful features like batched mapping of samples.\n","- [Huggingface Tokenizers](https://huggingface.co/docs/tokenizers/python/latest/): Library for text normalization and tokenization\n","- [Stanza](https://stanfordnlp.github.io/stanza/): Text processing toolkit from the Stanford NLP group\n","   - usefull for sentence splitting\n","\n","## Dataset\n","OpenI [[Website](https://openi.nlm.nih.gov/faq#collection)] [[Download](https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz)]\n","- Contains reports and scans\n","- We only use the reports from this dataset"]},{"cell_type":"markdown","metadata":{"id":"6zYPRTlpwVsj"},"source":["## Step 1 - Text Normalization"]},{"cell_type":"markdown","metadata":{"id":"pyjpv7e62Sg_"},"source":["In this step the goal is to normalize text from the corpus.\n","\n","TODO: Implement a function for normalizing a single text sample (e.g. a report from). This function will later be applied to all sample of the corpus.\n","You can decide which text normalization you find appropriate and implement one or some of them. Typical examples are:\n","- Unicode normalization\n","- Stripping accents\n","- Lowercasing\n","- Removing control characters\n","- Normalizing whitespace characters (i.e. replacing all whitespaces like tabs, ... by the default whitespace) and removing redundant whitespaces\n","- Removing or normalizing special characters\n","\n","Note: you may implement each normalizazion using regex/string operations or you can use normalizers from https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers"]},{"cell_type":"code","metadata":{"id":"ywVCYPWUwpl8"},"source":["from tokenizers.normalizers import *\n","\n","# TODO: implement normalize_text using regex/string operations and/or normalizers from tokenizers.normalizers\n","def normalize_text(text: str) -> str:\n","  return None\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# apply normalization to some example text\n","# you can play around with different example texts to understand the effects of each normalizer\n","example_text = 'SchÃ¶ner Tag'\n","\n","print(normalize_text(example_text))"],"metadata":{"id":"WHBm4H4NDZxe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x8XW3OmixY2y"},"source":["## Step 2 - Sentence Splitting"]},{"cell_type":"markdown","metadata":{"id":"Hot5HA7f2Po4"},"source":["In this step the goal is to split each section into sentences.\n","\n","TODO: Implement a function for splitting a single report section (given as a single string) into sentences (returned as a list of strings, one string for each sentence). This function will later be applied to all samples of the corpus.\n","For sentence splitting the stanza tokenizer should be used as it provides a more robust solution compared to splitting on pre-defined characters (like \".\") .\n"]},{"cell_type":"code","metadata":{"id":"wihSdqW52QKa"},"source":["import stanza\n","from typing import List\n","\n","stanza_tokenizer = stanza.Pipeline('en', processors='tokenize', use_gpu=True)\n","\n","def split_sentences(text: str) -> List[str]:\n","  # TODO: use the stanza_tokenizer to split the text string into sentences\n","  # See https://stanfordnlp.github.io/stanza/tokenize.html#tokenization-and-sentence-segmentation\n","  return None\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# apply sentence splitting on some example text\n","example_text = \"This is the first sentence. Let's try something else, e.g. this. Another sentence?\"\n","\n","print(split_sentences(example_text))"],"metadata":{"id":"q2sG1E5KupYh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4WHrFJhNxiJB"},"source":["## Step 3 - Apply text normalization and sentence splitting to the Open-I dataset (nothing TODO here)\n","We now apply text normalization and sentences splitting to the Open-I dataset.\n","Each sample of the Open-I dataset contains a report with (besides others) a \"findings\" and \"impression\" section.\n","\n","We make use of the huggingface datasets library for efficiently applying text normalization and sentence splitting to both sections of each sample of the Open-I dataset.\n","This step is already implemented but utilizes the functions implemented in Step 1 and 2.\n"]},{"cell_type":"markdown","metadata":{"id":"pCmeMqyu20h5"},"source":["1. Load the Open-I dataset as a huggingface Dataset"]},{"cell_type":"code","metadata":{"id":"MUiAQ9A52-iH"},"source":["from typing import List\n","import glob\n","from tqdm import tqdm\n","import xml.etree.ElementTree as ET\n","from datasets import Dataset\n","\n","def load_open_i_dataset():\n","  # Load from xml-files\n","  uIds = []\n","  findings_sections = []\n","  impression_sections = []\n","  for file in tqdm(glob.glob(\"open_i/ecgen-radiology/*.xml\")):\n","    tree = ET.parse(file)\n","\n","    uId = tree.find(\"./uId\").get('id')\n","    uIds.append(uId)\n","\n","    finding = tree.find(\".//AbstractText[@Label='FINDINGS']\").text\n","    findings_sections.append(finding if finding is not None else '')\n","\n","    imp = tree.find(\".//AbstractText[@Label='IMPRESSION']\").text\n","    impression_sections.append(imp if imp is not None else '')\n","\n","  assert len(uIds) > 0, 'No data found. Download the data first.'\n","  return Dataset.from_dict({\n","      'uId': uIds,\n","      'findings': findings_sections,\n","      'impression': impression_sections\n","  })\n","\n","data = load_open_i_dataset()\n","print(data) # Overview over the dataset\n","print(data[0])  # The first sample of the dataset\n","# Save to be loaded later\n","data.save_to_disk('open_i_raw')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D9V0DosV3VuP"},"source":["2. Apply normalize_text to the findings and impression sections using the map function of datasets.Dataset"]},{"cell_type":"code","metadata":{"id":"rbAjQch1_Z1J"},"source":["from datasets import load_from_disk, Dataset\n","\n","data: Dataset = load_from_disk('open_i_raw')\n","def normalize_text_samples(sample: dict) -> dict:\n","  return {\n","      'findings_normalized': normalize_text(sample['findings']),\n","      'impression_normalized': normalize_text(sample['impression'])\n","  }\n","data = data.map(normalize_text_samples)\n","\n","print(data) # Overview over the dataset\n","print(data[0])  # The first sample of the dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vSXv8QNeAn-b"},"source":["3. Apply split_sentences to the findings and impression sections using the map function of datasets.Dataset"]},{"cell_type":"code","metadata":{"id":"EY4-GRj3YiZb"},"source":["def split_sample_sentences(sample: dict) -> dict:\n","  return {\n","      'findings_sentences': split_sentences(sample['findings']),\n","      'impression_sentences': split_sentences(sample['impression'])\n","  }\n","\n","split_dataset = data.map(split_sample_sentences)\n","\n","print(split_dataset)\n","print(split_dataset[0])\n","split_dataset.save_to_disk('open_i_split')\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GA3p83wOwdy8"},"source":["## Step 4 - Vocabulary Creation: Train a tokenizer\n","In this step the goal is to learn a vocabulary (i.e. tokenizer) from the Open-I sentences. We therefore extract a list of all sentences (from findings and impression section) in the dataset. This list is then used for learning the vocabulary.\n","\n","TODO: Extract all sentences and store them in all_sentences."]},{"cell_type":"code","source":["from datasets import load_from_disk, Dataset\n","from typing import List\n","from tqdm import tqdm\n","\n","split_dataset: Dataset = load_from_disk('open_i_split')\n","print(split_dataset)\n","\n","# TODO: extract the list of all sentences in the whole dataset (from both sections)\n","all_sentences: List[str] = None\n","\n","print(f'\\nTotal number of sentences: {len(all_sentences)}')\n","print(f'Total number of words (by whitespaces): {sum(len(sent.split()) for sent in all_sentences)}')\n","num_unique_words = len(set(word for sent in all_sentences for word in sent.split()))\n","print(f'Total number of different words: {num_unique_words}')\n"],"metadata":{"id":"UNp27c98Fs4V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We no define the initial alphabet (optional, by default the alphabet is derived from the characters present in the dataset) and the size of the vocabulary we want to create."],"metadata":{"id":"uOa8C0_4WdOi"}},{"cell_type":"code","source":["import string\n","\n","# All ascii characters (lowercase only) + digits\n","initial_alphabet = list(string.ascii_lowercase) + list(string.digits)\n","print(f'Initial alphabet size: {len(initial_alphabet)}')\n","\n","vocab_size = 50000"],"metadata":{"id":"S8uzL_qe38is"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we use the huggingface tokenizer library to learn a WordPiece tokenizer. This is already implemented but you can try with different arguments or also try other tokenizers, e.g. BPE."],"metadata":{"id":"vfi-3JroWpCb"}},{"cell_type":"code","source":["\n","from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers, processors\n","\n","special_tokens = [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"]\n","tokenizer = Tokenizer(models.WordPiece(unk_token='<UNK>'))\n","# we use a lowercase vocabulary\n","tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n","tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n","tokenizer.post_processor = processors.TemplateProcessing(\n","    single=\"<BOS> $0 <EOS>\",\n","    pair=\"<BOS> $A <EOS> <BOS> $B <EOS>\",\n","    special_tokens=[(\"<BOS>\", 2), (\"<EOS>\", 3)],\n",")\n","tokenizer.decoder = decoders.WordPiece()\n","\n","trainer = trainers.WordPieceTrainer(\n","    vocab_size=vocab_size,\n","    initial_alphabet=initial_alphabet,\n","    special_tokens=special_tokens,\n",")\n","\n","# now train it\n","tokenizer.train_from_iterator(all_sentences, trainer=trainer)\n","\n","print(f'Vocab size: {tokenizer.get_vocab_size()}')\n","print('Vocab: \\n', tokenizer.get_vocab())\n","\n","tokenizer.save('tokenizer.json', pretty=True)"],"metadata":{"id":"wmWoQGHy3da2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Inspection of the created vocabulary\n","You can now take a look at the created vocabulary by opening the file \"tokenizer.json\".\n","\n","You can also try around tokenizing some samples:"],"metadata":{"id":"s5O4vgqeFtTN"}},{"cell_type":"code","source":["example_section = split_dataset[0]['findings_sentences']\n","example_sentence = example_section[0]\n","\n","print(example_sentence)\n","\n","encoded = tokenizer.encode(example_sentence)\n","print(f'Encoded tokens: {encoded.tokens}')\n","print(f'Encoded token ids: {encoded.ids}')\n","print(f'Decoded again: {tokenizer.decode(encoded.ids)}')\n","print(f'Decoded (keep special tokens): {tokenizer.decode(encoded.ids, skip_special_tokens=False)}')"],"metadata":{"id":"1YYXc8_t8mvF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fBPyy6E5wpVC"},"source":["# Task 2: Training a Text Classifier (graded!)\n","## Goals\n","- Understand how pre-trained language models can be utilized for downstream tasks\n","\n","## Tools\n","- [Huggingface Transformers](https://huggingface.co/transformers/): Library for pre-trained transformer-based language models)\n","\n","## Dataset\n","Medical Transcriptions [[Kaggle](https://www.kaggle.com/tboyle10/medicaltranscriptions)]"]},{"cell_type":"markdown","metadata":{"id":"9olvk3WNS5Uc"},"source":["## Step 1 - Load the Medical Transctions Dataset (Nothing TODO here)\n","We first load the Medical Transcriptions dataset as a huggingface dataset. The dataset is split into train and test set, so the returned type is a datasets.DatasetDict, which acts like a dictionary of datasets.Dataset but provides utility functions, e.g. for mapping all datasets of the dict using the map method.\n","This step is already implemented."]},{"cell_type":"code","metadata":{"id":"e05T0u7US1jt"},"source":["from typing import List, Tuple\n","import pandas as pd\n","from datasets import Dataset, DatasetDict\n","from sklearn.model_selection import train_test_split\n","\n","def load_transciptions_dataset() -> Tuple[DatasetDict, List[str]]:\n","    df = pd.read_csv('medical_transcriptions/mtsamples.csv')\n","    df = df.drop(['Unnamed: 0'],axis=1,)\n","\n","    counts = df['medical_specialty'].value_counts()\n","    print(f'Original counts:\\n{counts}')\n","    dropped_specialities = [k for k, v in counts.items() if v < 100]\n","    for dropped_speciality in dropped_specialities:\n","      df = df[df['medical_specialty'] != dropped_speciality]\n","    df.dropna(inplace=True)\n","    counts = df['medical_specialty'].value_counts()\n","    print(f'Counts after removing small specialities:\\n{counts}')\n","\n","    df['medical_specialty'] = df['medical_specialty'].astype('category')\n","    class_names = df['medical_specialty'].cat.categories.tolist()\n","    print(f'Class names : {class_names}')\n","    df['medical_specialty'] = df['medical_specialty'].cat.codes\n","\n","    train, test = train_test_split(df, stratify=df['medical_specialty'], test_size=0.25)\n","    dataset = DatasetDict({'train': Dataset.from_pandas(train), 'test': Dataset.from_pandas(test)})\n","    dataset = dataset.remove_columns(['__index_level_0__'])\n","\n","    return dataset, class_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset, class_names = load_transciptions_dataset()"],"metadata":{"id":"iKtCzBSp1ADK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Training samples: ', len(dataset['train']['transcription']))\n","print('Test samples: ', len(dataset['test']['transcription']))\n","print('Columns: ', dataset['train'].features)"],"metadata":{"id":"NMwK0iiUvhUV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zKthj9SypvMB"},"source":["## Step 2 - Compute sentence embeddings using pre-trained language model\n","Our goal is to use a pre-trained language model (from the huggingface transformers library) to encode sentences into sentence representations. Therefore, each sentence is tokenized and passed to the language model. The resulting contextualized token representations (i.e. the outputs of the last hidden layer of the language model) are then globally pooled to get sentence-level representations. As we use a pre-trained model, no training is involved in this step."]},{"cell_type":"markdown","source":["### Specify pre-trained language model\n","TODO: Decide which pre-trained language model you want to use and specify its name here.\n","You can search for models at the huggingface model hub: https://huggingface.co/models . You can either use standard models, e.g. BERT, or use biomedcial models.\n","\n","When you decided for a model, copy the name of the model from the URL (removing only https://huggingface.co/) and insert it as model_name here.\n","For more reference on how to use the tokenizer have a look at https://huggingface.co/docs/transformers/preprocessing."],"metadata":{"id":"kTggsRsJHxCM"}},{"cell_type":"code","source":["# TODO: Insert model name here\n","model_name = None"],"metadata":{"id":"y1h6SqqcHwIZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the tokenizer (nothing TODO)\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","print(tokenizer)"],"metadata":{"id":"k85a1cxn1RSG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the model (nothing TODO) and inspect it\n","from transformers import AutoModel\n","model = AutoModel.from_pretrained(model_name)\n","print(model)"],"metadata":{"id":"YSaLG4Y6H113"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Experiment with tokenizer and language model (nothing to implement here)"],"metadata":{"id":"-U1wMp2SHInc"}},{"cell_type":"code","metadata":{"id":"Aipe5EhsTaIf"},"source":["# Tokenize some example text and inspect the results to understand the outputs of the tokenizer\n","sample_text = dataset['train'][:2]['transcription']  # batch of 2 samples of text\n","print('example text: ', sample_text)\n","toknized = tokenizer(sample_text, truncation=True, max_length=512)\n","print(toknized.keys())\n","print('input ids: ', toknized['input_ids'])\n","print('attention mask: ', toknized['attention_mask'])\n","print('length: ', len(toknized['input_ids'][0]))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feed some example data to understand the outputs of the language model\n","\n","# pad the batch and convert it to Pytorch\n","# return_tensors='pt' => return the tokenized values as PyTorch tensors instead of lists\n","x = tokenizer.pad(toknized, return_tensors='pt')\n","print(x.keys())\n","print('input ids: ', x['input_ids'])\n","print('attention mask: ', x['attention_mask'])\n","print('shape: ', x['input_ids'].shape)\n","\n","# Encode the tokenized and padded input\n","results = model(**x)\n","print('output shape: ', results.last_hidden_state.shape)  # (batch_size x num_tokens x d_hidden)"],"metadata":{"id":"XK7Zzjyp3yJ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Tokenize the dataset\n","We now tokenize the whole dataset using the batched map-method.\n","\n","TODO: Implement the tokenize_batch function using the tokenizer. Note: do not pad the data yet but truncate it to a max length of 512."],"metadata":{"id":"XrqTaJHVEF0r"}},{"cell_type":"code","source":["def tokenize_batch(text_batch: List[str]):\n","  # TODO: implement tokenize batch using the tokenizer\n","  return None"],"metadata":{"id":"MN1mFK1uEHeE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply tokenize_batch to dataset (nothing TODO here)\n","tokenized_dataset = dataset.map(lambda examples: tokenize_batch(examples[\"transcription\"]), batched=True)\n","tokenized_dataset = tokenized_dataset.remove_columns(['sample_name', 'transcription', 'description', 'keywords']).rename_column('medical_specialty', 'labels')\n","print('Columns after tokenization', list(tokenized_dataset['train'].features.keys()))"],"metadata":{"id":"J9VfStgkEip8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Token Pooling\n","Define Pooling functions to compute sentence embeddings from token outputs.\n","\n","TODO: implement three possible pooling functions\n","- CLS: use the output of the [CLS] token only and ignore the other tokens.\n","- max/avg: globally max/avg pool over all token outputs, but make sure to ignore padding tokens."],"metadata":{"id":"bVq84WgQG9BR"}},{"cell_type":"code","source":["import torch\n","\n","def CLS_pool(hidden_state: torch.FloatTensor, attention_mask: torch.BoolTensor):\n","  \"\"\"\n","  Returns only the hidden state of the [CLS] token\n","  :param hidden_state: (N x M x d_hidden) where N is the batch size and M is the number of tokens\n","  :param attention_mask: (N x M), True for \"real\" tokens, False for padding tokens.\n","  :return (N x d_hidden)\n","  \"\"\"\n","  # TODO: implement function\n","  return None\n","\n","def max_pool(hidden_state: torch.FloatTensor, attention_mask: torch.BoolTensor):\n","  \"\"\"\n","  Globally pools the hidden states over all tokens using max pooling.\n","  :param hidden_state: (N x M x d_hidden) where N is the batch size and M is the number of tokens\n","  :param attention_mask: (N x M), True for \"real\" tokens, False for padding tokens.\n","  :return (N x d_hidden)\n","  \"\"\"\n","  # TODO: implement function\n","  return None\n","\n","def avg_pool(hidden_state: torch.FloatTensor, attention_mask: torch.BoolTensor):\n","  \"\"\"\n","  Globally pools the hidden states over all tokens using average pooling.\n","  :param hidden_state: (N x M x d_hidden) where N is the batch size and M is the number of tokens\n","  :param attention_mask: (N x M), True for \"real\" tokens, False for padding tokens.\n","  :return (N x d_hidden)\n","  \"\"\"\n","  # TODO: implement function\n","  return None"],"metadata":{"id":"bQzwuFxMGax9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sentence Embedder\n","The sentence embedder takes a tokenized input, uses the language model to compute token representations (i.e. the last hidden_state of the language model) and then uses a pooling function to compute a single sentence representation."],"metadata":{"id":"O7jf0k3_H5nQ"}},{"cell_type":"markdown","source":["TODO: implement the sentence embedder forward method."],"metadata":{"id":"nUZs8TqU2Gnl"}},{"cell_type":"code","source":["from torch import nn\n","from typing import Dict\n","\n","class SentenceEmbedder(nn.Module):\n","  def __init__(self, model, pool):\n","    super().__init__()\n","    self.model = model  # this is a huggingface language model from AutoModel.from_pretrained\n","    self.pool = pool  # this is one of the three defined pooling functions (CLS, max, avg)\n","\n","  def forward(self, x: Dict[str, torch.Tensor]) -> torch.Tensor:\n","    \"\"\"\n","    :param x: dict containing the following elements:\n","      - input_ids: torch.Tensor of the token_type ids with shape (N x M)\n","      - attention_mask: torch.Tensor containing the attention mask of shape (N x M)\n","    :return sentence_embedding for each sample of shape (N x d_hidden)\n","    \"\"\"\n","    # TODO: implement forward\n","\n","    return None\n"],"metadata":{"id":"SLyFxgkTIA5h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TODO: now decide which pooling function you want to use"],"metadata":{"id":"SAIiUT9D2Iu9"}},{"cell_type":"code","source":["# TODO: try different pooling functions (nothing more to implement here)\n","# (also change the dataset_name so you can later easily switch between pooling functions without the need to recompute the embeddings)\n","device = 'cuda:0'\n","dataset_name = 'encoded_transciptions_CLS'\n","pool = CLS_pool\n","\n","sentence_embedder = SentenceEmbedder(model, pool).to(device=device)\n","\n"],"metadata":{"id":"w6J8KflrZFAj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Now run the sentence embedder (nothing TODO here).\n","\n"," The resulting dataset will then contain a sentence_embedding column."],"metadata":{"id":"miiKNXPV2OS8"}},{"cell_type":"code","source":["def embed_sentence(batch):\n","  model_input = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}\n","  model_input = tokenizer.pad(model_input, return_tensors='pt').to(device=device)\n","\n","  with torch.no_grad():\n","    sentence_embeddings = sentence_embedder(model_input)\n","  return {'sentence_embedding': sentence_embeddings.detach().cpu().numpy()}\n","\n","encoded_dataset = tokenized_dataset.map(embed_sentence, batched=True, batch_size=64)\n","encoded_dataset.save_to_disk(dataset_name)"],"metadata":{"id":"M-q6nYkT0VkX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4QzqwS5rJCK9"},"source":["## Step 5 - Train Classification Model on Sentence Embeddings\n","Now we have a single sentence representation vector for each sentence. We now learn a simple classifier based on a MLP (i.e. a 2-layer fully-connected neural network). We first project each sentence embedding from d_hidden to d_mlp, apply ReLU (or another non-linearity) and then project the resulting vector into num_classes where we then apply the cross entropy loss. The classification head will then be learned based on the training data. This is the only learned component in our model.\n","\n","TODO: implement the MLP head"]},{"cell_type":"code","metadata":{"id":"LNnx-9QzsIng"},"source":["from torch import nn\n","import datasets\n","from datasets import load_from_disk\n","from torch.utils.data import DataLoader\n","import torch\n","from tqdm import tqdm\n","\n","class ClassificationHead(nn.Module):\n","  def __init__(self, d_hidden: int, d_mlp: int, num_classes: int):\n","    super().__init__()\n","    # TODO: add the required layers here\n","    self.mlp_head = None\n","\n","    # TODO: define the loss function to use here\n","    self.loss = None\n","\n","  def forward(self, x, y_true):\n","    \"\"\"\n","    :param x: sentence embeddings (N x d_hidden)\n","    :param y_true: target classes (multiclass) (N)\n","    \"\"\"\n","    # TODO: apply MLP head to sentence embeddings\n","    # (N x num_classes)\n","    logits = None\n","\n","    # TODO: apply the loss function\n","    loss = None\n","    # TODO: compute the predictions (i.e. target classes as longs) from the logits\n","    # (N)\n","    y_pred = None\n","\n","    return y_pred, loss\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now train the classification head on the sentence embeddings.\n","The training is already implemented.\n","\n","TODO: specify and try different hyperparameters"],"metadata":{"id":"DTTplEVL3g_g"}},{"cell_type":"code","source":["# TODO: try different hyperparameters\n","device = 'cuda:0'\n","dataset_name = 'encoded_transciptions_CLS'\n","num_epochs = 20\n","lr = 1e-4\n","weight_decay = 1e-5\n","d_hidden = 768  # must match the language model hidden output\n","d_mlp = 1024\n","batch_size = 64"],"metadata":{"id":"ZK_cuGPn1jUZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5MXXBwIyqAjU"},"source":["# Now run the training...\n","\n","print('num classes: ', len(class_names))\n","encoded_dataset = load_from_disk(dataset_name)\n","# make sure PyTorch Tensors are returned\n","encoded_dataset.set_format('pt')\n","\n","# remove all columns except sentence_embedding and labels\n","columns_to_remove = set(encoded_dataset['train'].column_names) - {'sentence_embedding', 'labels'}\n","encoded_dataset = encoded_dataset.remove_columns(columns_to_remove)\n","\n","train_data_loader = DataLoader(encoded_dataset['train'], batch_size=batch_size, shuffle=True)\n","test_data_loader = DataLoader(encoded_dataset['test'], batch_size=batch_size, shuffle=False)\n","\n","classification_model = ClassificationHead(d_hidden=d_hidden,\n","                                          d_mlp=d_mlp,\n","                                          num_classes=len(class_names))\n","classification_model = classification_model.to(device=device)\n","optimizer = torch.optim.Adam(classification_model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","classification_model.train()\n","print('Training...')\n","for epoch in range(num_epochs):\n","  train_loss = []\n","  for train_batch in tqdm(train_data_loader):\n","    x = train_batch['sentence_embedding'].to(device=device)\n","    y_true = train_batch['labels'].to(device=device)\n","\n","    y_pred, loss = classification_model(x, y_true)\n","    train_loss.append(loss.item())\n","\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","  print('train loss: ', torch.mean(torch.tensor(train_loss)).item())\n","\n","classification_model.eval()\n","print('Testing...')\n","f1_metric = datasets.load_metric(\"f1\")\n","with torch.no_grad():\n","  test_loss = []\n","  for test_batch in tqdm(test_data_loader):\n","    x = test_batch['sentence_embedding'].to(device=device)\n","    y_true = test_batch['labels'].to(device=device)\n","\n","    y_pred, loss = classification_model(x, y_true)\n","\n","    f1_metric.add_batch(predictions=y_pred, references=y_true)\n","    test_loss.append(loss)\n","print('\\ntest loss: ', torch.mean(torch.tensor(test_loss)).item())\n","print('F1 (Macro): ', f1_metric.compute(average=\"macro\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Task 3: Generating text from a pre-trained decoder LM (graded!)\n","## Goals\n","- Understand how inference (text generation) works on decoder languages models\n","\n","## Tools\n","- [Huggingface Transformers](https://huggingface.co/transformers/): Library for pre-trained transformer-based language models)\n","\n","## Model\n","- Architecture: https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel\n","- Model weights: https://huggingface.co/healx/gpt-2-pubmed-medium\n","\n","## Notes and Tips\n","- You can read this blog post to learn more about generation strategies: https://huggingface.co/blog/how-to-generate\n","- We are going to implement greedy-search.\n","- Please do not use the \"generate\" method of the pre-trained model (as there generation is already implemented)\n","- You can however take a look at their implementation of greedy-search: https://github.com/huggingface/transformers/blob/04ab5605fbb4ef207b10bf2772d88c53fc242e83/src/transformers/generation/utils.py#L2080\n"],"metadata":{"id":"RyGeE0JKWmJN"}},{"cell_type":"markdown","source":["## Step 1: Implement generate method\n","Implement the generate method that takes a language model and its tokenizer together with a list of text prefixes and outputs a list of generated sentences (one for each prefix). The prefixes should be \"autocompleted\" by the model.\n","Use the greedy-search method and process all prefixes as a single batch.\n","\n","TODO: finish the implementation of the generate method by filling in the missing lines."],"metadata":{"id":"IlHUnMYEY49m"}},{"cell_type":"code","source":["import torch\n","from typing import List\n","\n","def generate(model, tokenizer, prefix: List[str], max_predicted: int) -> List[str]:\n","  \"\"\"\n","  :param model: PreTrainedModel.\n","  :param tokenizer: PreTrainedTokenizer (https://huggingface.co/docs/transformers/main/main_classes/tokenizer#transformers.PreTrainedTokenizer)\n","  :prefix: Batch of prefixes to be autocompleted by the language model\n","  \"\"\"\n","  if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","  pad_token_id = tokenizer.pad_token_id\n","  bos_token_id = tokenizer.bos_token_id\n","  eos_token_id = tokenizer.eos_token_id\n","  device = model.device\n","\n","  # 1. Tokenize the prefixes and prepare them for input into the language model\n","  #    Notes\n","  #    - add the start token (BOS) but not the end token (EOS)\n","  #    - left! pad them to the maximum length in the batch, i.e. paddding is left to the \"real\" tokens\n","  #    - inputs_ids and attention_mask should be computes, and should already be stacked in the batch dim\n","  #    -> this can all be done by calling the tokenizer! See: https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__\n","  tokenizer.padding_side = 'left'\n","  # TODO: call the tokenizer to prepare the inputs (a dictionary)\n","  inputs = tokenizer(...  # add arguments here\n","                     return_tensors='pt')\n","  inputs = inputs.to(device=device)\n","  input_ids = inputs['input_ids']\n","  attention_mask = inputs['attention_mask']\n","\n","  # TODO: Compute position ids based on the attention mask\n","  # Note: the position ids are the indices of the positions, starting with zero and increasing for each non-padding token\n","  # The positions ids for padding tokens can take any value\n","  position_ids: torch.LongTensor = None\n","\n","  # Initialize some variables (nothing TODO here)\n","  N = position_ids.shape[0]\n","  past_key_values = None\n","  unfinished_sequences = torch.ones(N, dtype=torch.bool, device=device)\n","  predicted_input_ids = input_ids.clone()\n","\n","  while True:\n","    # 2. Predict the next token logits by passing the previous inputs into the language model\n","    #    Note: already computed steps can be given by past_key_values, other steps are given as input_ids\n","    outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask,\n","        position_ids=position_ids,\n","        past_key_values=past_key_values,\n","        return_dict=True\n","    )\n","    # (N x |V|)\n","    next_token_logits: torch.LongTensor = outputs.logits[:, -1, :]\n","\n","    # 3. Select the next predicted tokens by greedy search\n","    # (N)\n","    next_tokens: torch.LongTensor = None  # TODO\n","\n","    # 4. For all already finished sentences, replace the next_tokens by <PAD>\n","    next_tokens = None  # TODO\n","\n","    # 5. Check which sentences are already finished by checking whether they contain <END>\n","    # Which sentences have been finished with the predicted token (next_tokens)\n","    # (N)\n","    newly_finished = None  # TODO\n","    # Which sentences, therefore, remain unfinished\n","    # (N)\n","    unfinished_sequences = None  # TODO\n","\n","    # 6. Concatenate the next input to predicted_input_ids, attention_mask\n","    # (N x M_new) where M_new is one longer than the previous M of predicted_input_ids\n","    predicted_input_ids = None  # TODO\n","    # (N x M_new)\n","    attention_mask = None  # TODO: extend attention mask by ones for next step\n","    # Nothing TODO here\n","    position_ids = position_ids.amax(dim=1, keepdim=True) + 1\n","    input_ids = next_tokens.unsqueeze(dim=1)\n","    past_key_values = outputs.past_key_values\n","\n","    # 7. Check if finished (nothing TODO here)\n","    if predicted_input_ids.shape[1] >= max_predicted or unfinished_sequences.max() == 0:\n","      break\n","\n","  # 8. Convert back to text (nothing TODO here)\n","  generated_sentences: List[str] = tokenizer.batch_decode(predicted_input_ids, skip_special_tokens=True)\n","  return generated_sentences\n","\n","\n"],"metadata":{"id":"RTkJ3hZqZsAK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2: Load a decoder language model (nothing TODO here)"],"metadata":{"id":"qQzhPO9Bx6bx"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","decoder_model_name = \"gpt2-medium\"\n","\n","decoder_tokenizer = AutoTokenizer.from_pretrained(decoder_model_name)\n","decoder_model = AutoModelForCausalLM.from_pretrained(decoder_model_name)\n","decoder_model = decoder_model.to(device=\"cuda:0\")"],"metadata":{"id":"WSObYXZCZ77g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 3: Apply the generate method to some example text (nothing TODO here)"],"metadata":{"id":"q3zq42ZlyQpU"}},{"cell_type":"code","source":["# You can play around with different texts here\n","prefix_1 = \"This sentences is about\"\n","prefix_2 = \"Can you complete this sentence?\"\n","prefixes = [prefix_1, prefix_2]\n","\n","generated_sentences = generate(decoder_model, decoder_tokenizer, prefixes, max_predicted=50)\n","\n","for i, (prefix, sent) in enumerate(zip(prefixes, generated_sentences)):\n","  print(f'Completed sentence {i}: \"{prefix}\"')\n","  print(\"-------------------------------------------------------\\n\")\n","  print(sent)\n","  print(\"\\n=======================================================\\n\\n\")"],"metadata":{"id":"sN8GuYvhyYdk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see, the results tend to be quite repetetive.\n","This is why greedy search is typically not used in practice.\n","Common alternatives include beam search and sampling from the distribution of next tokens. While this is not part of the mandatory exercise, you can try to implement other generation methods as part of a bonus task and see how it can improve the results."],"metadata":{"id":"_M43bZ4s9UCH"}},{"cell_type":"markdown","source":["## Extra Task (not graded): Implement a beam search generate function and apply it\n","Re-implement the generate method but instead of doing greedy search, use beam search instead.\n","\n","Note that this is a completely optional and non-graded task and will not be discussed in the soutions!"],"metadata":{"id":"VWu4pDaJy2XG"}},{"cell_type":"code","source":["def beam_search_generate(model, tokenizer, prefix: List[str], max_predicted: int, num_beams: int = 5) -> List[str]:\n","  pass  # TODO"],"metadata":{"id":"zewLo1VLzfpg"},"execution_count":null,"outputs":[]}]}