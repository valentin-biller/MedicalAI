{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ9sen2-10wB"
   },
   "source": [
    "# Hand-in Group\n",
    "TODO: State the names of all group members and TUM-IDs\n",
    "\n",
    "* Bhat Maitri: 03756699\n",
    "* Biller Valentin: 03724152\n",
    "* Szabo Daniel: 03726951"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dz_Is5Bw_PZC"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ALExTEN17Jv"
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3ZN2hZNGvkBd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Downloading numpy-1.26.4-cp39-cp39-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-16.1.0-cp39-cp39-macosx_10_15_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.2-cp39-cp39-macosx_10_9_x86_64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.19.0 (from datasets)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.62.1 (from datasets)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.5-cp39-cp39-macosx_10_9_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from datasets) (24.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached PyYAML-6.0.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp39-cp39-macosx_10_9_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp39-cp39-macosx_10_9_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->datasets)\n",
      "  Using cached charset_normalizer-3.3.2-cp39-cp39-macosx_10_9_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->datasets)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->datasets)\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from pandas->datasets) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.5-cp39-cp39-macosx_10_9_x86_64.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.6/401.6 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp39-cp39-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.1.0-cp39-cp39-macosx_10_15_x86_64.whl (28.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.4/28.4 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.1-cp39-cp39-macosx_10_9_x86_64.whl (197 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp39-cp39-macosx_10_9_x86_64.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-macosx_10_9_x86_64.whl (31 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached charset_normalizer-3.3.2-cp39-cp39-macosx_10_9_x86_64.whl (122 kB)\n",
      "Downloading frozenlist-1.4.1-cp39-cp39-macosx_10_9_x86_64.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.2/55.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp39-cp39-macosx_10_9_x86_64.whl (30 kB)\n",
      "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp39-cp39-macosx_10_9_x86_64.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.7/83.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, xxhash, urllib3, tzdata, tqdm, pyyaml, pyarrow-hotfix, numpy, multidict, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, pyarrow, pandas, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 datasets-2.19.1 dill-0.3.8 filelock-3.14.0 frozenlist-1.4.1 fsspec-2024.3.1 huggingface-hub-0.23.0 idna-3.7 multidict-6.0.5 multiprocess-0.70.16 numpy-1.26.4 pandas-2.2.2 pyarrow-16.1.0 pyarrow-hotfix-0.6 pytz-2024.1 pyyaml-6.0.1 requests-2.31.0 tqdm-4.66.4 tzdata-2024.1 urllib3-2.2.1 xxhash-3.4.1 yarl-1.9.4\n",
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.19.1-cp39-cp39-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from tokenizers) (0.23.0)\n",
      "Requirement already satisfied: filelock in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\n",
      "Downloading tokenizers-0.19.1-cp39-cp39-macosx_10_12_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.19.1\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.10-cp39-cp39-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp39-cp39-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.10-cp39-cp39-macosx_10_9_x86_64.whl (281 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.7/281.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp39-cp39-macosx_10_12_x86_64.whl (416 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.1/416.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, transformers\n",
      "Successfully installed regex-2024.5.10 safetensors-0.4.3 transformers-4.40.2\n",
      "Collecting stanza\n",
      "  Downloading stanza-1.8.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Downloading emoji-2.11.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from stanza) (1.26.4)\n",
      "Collecting protobuf>=3.15.0 (from stanza)\n",
      "  Downloading protobuf-5.26.1-cp37-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from stanza) (2.31.0)\n",
      "Collecting networkx (from stanza)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting toml (from stanza)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting torch>=1.3.0 (from stanza)\n",
      "  Downloading torch-2.2.2-cp39-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tqdm in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from stanza) (4.66.4)\n",
      "Requirement already satisfied: filelock in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from torch>=1.3.0->stanza) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from torch>=1.3.0->stanza) (4.11.0)\n",
      "Collecting sympy (from torch>=1.3.0->stanza)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting jinja2 (from torch>=1.3.0->stanza)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: fsspec in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from torch>=1.3.0->stanza) (2024.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from requests->stanza) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from requests->stanza) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from requests->stanza) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages (from requests->stanza) (2024.2.2)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.3.0->stanza)\n",
      "  Downloading MarkupSafe-2.1.5-cp39-cp39-macosx_10_9_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.3.0->stanza)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading stanza-1.8.2-py3-none-any.whl (990 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.1/990.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.26.1-cp37-abi3-macosx_10_9_universal2.whl (404 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.0/404.0 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp39-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading emoji-2.11.1-py2.py3-none-any.whl (433 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Downloading MarkupSafe-2.1.5-cp39-cp39-macosx_10_9_x86_64.whl (14 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, toml, sympy, protobuf, networkx, MarkupSafe, emoji, jinja2, torch, stanza\n",
      "Successfully installed MarkupSafe-2.1.5 emoji-2.11.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.2.1 protobuf-5.26.1 stanza-1.8.2 sympy-1.12 toml-0.10.2 torch-2.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dani/opt/anaconda3/envs/ai_med2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 153MB/s]                     \n",
      "2024-05-15 18:01:03 INFO: Downloaded file to /Users/dani/stanza_resources/resources.json\n",
      "2024-05-15 18:01:03 INFO: Downloading default packages for language: en (English) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/default.zip: 100%|██████████| 527M/527M [00:16<00:00, 32.3MB/s] \n",
      "2024-05-15 18:01:21 INFO: Downloaded file to /Users/dani/stanza_resources/en/default.zip\n",
      "2024-05-15 18:01:24 INFO: Finished downloading models and saved to /Users/dani/stanza_resources\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install tokenizers\n",
    "!pip install transformers\n",
    "!pip install stanza\n",
    "# -- Initialize Stanza --\n",
    "import stanza\n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGa7gWpX2ArF"
   },
   "source": [
    "## Download Datasets\n",
    "Note: In case the following script fails because the medical transcriptions dataset is no longer available it can be downloaded from https://www.kaggle.com/tboyle10/medicaltranscriptions ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UGhgMCGa2C7F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 3955 files from the OpenI dataset into \"open_i/ecgen-radiology\"\n",
      "zsh:1: command not found: gdown\n",
      "unzip:  cannot find or open medical_transcriptions/medical_transcriptions.zip, medical_transcriptions/medical_transcriptions.zip.zip or medical_transcriptions/medical_transcriptions.zip.ZIP.\n",
      "Downloaded medical transriptions dataset\n"
     ]
    }
   ],
   "source": [
    "# -- OPEN-I dataset --\n",
    "!mkdir -p open_i\n",
    "!wget -q -N -P open_i https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz\n",
    "!tar zxf open_i/NLMCXR_reports.tgz -C open_i\n",
    "import glob\n",
    "num_files = len(glob.glob(\"open_i/ecgen-radiology/*.xml\"))\n",
    "print(f'Downloaded {num_files} files from the OpenI dataset into \"open_i/ecgen-radiology\"')\n",
    "\n",
    "# -- Transcriptions Dataset --\n",
    "!mkdir -p medical_transcriptions\n",
    "!gdown --id 1E0hm3r9bwK8cujyIcOjp_y-ZEPt1HBjn -O medical_transcriptions/medical_transcriptions.zip\n",
    "!unzip -o medical_transcriptions/medical_transcriptions.zip -d medical_transcriptions\n",
    "print(f'Downloaded medical transriptions dataset')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbcFrCre_K84"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I62jANWNvkzp"
   },
   "source": [
    "# Task 1: Pre-Processing and Vocabulary Construction (not graded!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7Oy5fnA1iwl"
   },
   "source": [
    "## Note\n",
    "- Task 1 is not graded, i.e. it is not mandatory to solve this task. It insteads serves soleley to show how pre-processing and tokenization can be done.\n",
    "- Note that tasks 2 and 3 are mandatory!\n",
    "\n",
    "## Goals\n",
    "- Understand tools for text pre-processing\n",
    "- Apply text normalization and sentence splitting to a medical corpus\n",
    "- Train a tokenizer on a medical corpus\n",
    "\n",
    "## Tools\n",
    "- [Huggingface Datasets](https://huggingface.co/docs/datasets/): Library for efficient loading, saving and processing of text datasets\n",
    "  - Note: In many cases using pandas is sufficient for text datasets (as they often fit into memory), but Hugginface Datasets provides some usful features like batched mapping of samples.\n",
    "- [Huggingface Tokenizers](https://huggingface.co/docs/tokenizers/python/latest/): Library for text normalization and tokenization\n",
    "- [Stanza](https://stanfordnlp.github.io/stanza/): Text processing toolkit from the Stanford NLP group\n",
    "   - usefull for sentence splitting\n",
    "\n",
    "## Dataset\n",
    "OpenI [[Website](https://openi.nlm.nih.gov/faq#collection)] [[Download](https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz)]\n",
    "- Contains reports and scans\n",
    "- We only use the reports from this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zYPRTlpwVsj"
   },
   "source": [
    "## Step 1 - Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyjpv7e62Sg_"
   },
   "source": [
    "In this step the goal is to normalize text from the corpus.\n",
    "\n",
    "TODO: Implement a function for normalizing a single text sample (e.g. a report from). This function will later be applied to all sample of the corpus.\n",
    "You can decide which text normalization you find appropriate and implement one or some of them. Typical examples are:\n",
    "- Unicode normalization\n",
    "- Stripping accents\n",
    "- Lowercasing\n",
    "- Removing control characters\n",
    "- Normalizing whitespace characters (i.e. replacing all whitespaces like tabs, ... by the default whitespace) and removing redundant whitespaces\n",
    "- Removing or normalizing special characters\n",
    "\n",
    "Note: you may implement each normalizazion using regex/string operations or you can use normalizers from https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ywVCYPWUwpl8"
   },
   "outputs": [],
   "source": [
    "from tokenizers.normalizers import *\n",
    "\n",
    "# TODO: implement normalize_text using regex/string operations and/or normalizers from tokenizers.normalizers\n",
    "def normalize_text(text: str) -> str:\n",
    "  return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WHBm4H4NDZxe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# apply normalization to some example text\n",
    "# you can play around with different example texts to understand the effects of each normalizer\n",
    "example_text = 'Schöner Tag'\n",
    "\n",
    "print(normalize_text(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8XW3OmixY2y"
   },
   "source": [
    "## Step 2 - Sentence Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hot5HA7f2Po4"
   },
   "source": [
    "In this step the goal is to split each section into sentences.\n",
    "\n",
    "TODO: Implement a function for splitting a single report section (given as a single string) into sentences (returned as a list of strings, one string for each sentence). This function will later be applied to all samples of the corpus.\n",
    "For sentence splitting the stanza tokenizer should be used as it provides a more robust solution compared to splitting on pre-defined characters (like \".\") .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wihSdqW52QKa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 18:01:29 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 26.9MB/s]                    \n",
      "2024-05-15 18:01:29 INFO: Downloaded file to /Users/dani/stanza_resources/resources.json\n",
      "2024-05-15 18:01:29 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-05-15 18:01:29 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "2024-05-15 18:01:29 WARNING: GPU requested, but is not available!\n",
      "2024-05-15 18:01:29 INFO: Using device: cpu\n",
      "2024-05-15 18:01:29 INFO: Loading: tokenize\n",
      "2024-05-15 18:01:30 INFO: Loading: mwt\n",
      "2024-05-15 18:01:30 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from typing import List\n",
    "\n",
    "stanza_tokenizer = stanza.Pipeline('en', processors='tokenize', use_gpu=True)\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "  # TODO: use the stanza_tokenizer to split the text string into sentences\n",
    "  # See https://stanfordnlp.github.io/stanza/tokenize.html#tokenization-and-sentence-segmentation\n",
    "  return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q2sG1E5KupYh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# apply sentence splitting on some example text\n",
    "example_text = \"This is the first sentence. Let's try something else, e.g. this. Another sentence?\"\n",
    "\n",
    "print(split_sentences(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WHrFJhNxiJB"
   },
   "source": [
    "## Step 3 - Apply text normalization and sentence splitting to the Open-I dataset (nothing TODO here)\n",
    "We now apply text normalization and sentences splitting to the Open-I dataset.\n",
    "Each sample of the Open-I dataset contains a report with (besides others) a \"findings\" and \"impression\" section.\n",
    "\n",
    "We make use of the huggingface datasets library for efficiently applying text normalization and sentence splitting to both sections of each sample of the Open-I dataset.\n",
    "This step is already implemented but utilizes the functions implemented in Step 1 and 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCmeMqyu20h5"
   },
   "source": [
    "1. Load the Open-I dataset as a huggingface Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MUiAQ9A52-iH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3955/3955 [00:00<00:00, 8111.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['uId', 'findings', 'impression'],\n",
      "    num_rows: 3955\n",
      "})\n",
      "{'uId': 'CXR162', 'findings': 'Heart size normal. Lungs are clear. XXXX are normal. No pneumonia, effusions, edema, pneumothorax, adenopathy, nodules or masses.', 'impression': 'Normal chest'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3955/3955 [00:00<00:00, 739368.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_open_i_dataset():\n",
    "  # Load from xml-files\n",
    "  uIds = []\n",
    "  findings_sections = []\n",
    "  impression_sections = []\n",
    "  for file in tqdm(glob.glob(\"open_i/ecgen-radiology/*.xml\")):\n",
    "    tree = ET.parse(file)\n",
    "\n",
    "    uId = tree.find(\"./uId\").get('id')\n",
    "    uIds.append(uId)\n",
    "\n",
    "    finding = tree.find(\".//AbstractText[@Label='FINDINGS']\").text\n",
    "    findings_sections.append(finding if finding is not None else '')\n",
    "\n",
    "    imp = tree.find(\".//AbstractText[@Label='IMPRESSION']\").text\n",
    "    impression_sections.append(imp if imp is not None else '')\n",
    "\n",
    "  assert len(uIds) > 0, 'No data found. Download the data first.'\n",
    "  return Dataset.from_dict({\n",
    "      'uId': uIds,\n",
    "      'findings': findings_sections,\n",
    "      'impression': impression_sections\n",
    "  })\n",
    "\n",
    "data = load_open_i_dataset()\n",
    "print(data) # Overview over the dataset\n",
    "print(data[0])  # The first sample of the dataset\n",
    "# Save to be loaded later\n",
    "data.save_to_disk('open_i_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9V0DosV3VuP"
   },
   "source": [
    "2. Apply normalize_text to the findings and impression sections using the map function of datasets.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rbAjQch1_Z1J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3955/3955 [00:00<00:00, 25894.89 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['uId', 'findings', 'impression', 'findings_normalized', 'impression_normalized'],\n",
      "    num_rows: 3955\n",
      "})\n",
      "{'uId': 'CXR162', 'findings': 'Heart size normal. Lungs are clear. XXXX are normal. No pneumonia, effusions, edema, pneumothorax, adenopathy, nodules or masses.', 'impression': 'Normal chest', 'findings_normalized': None, 'impression_normalized': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "data: Dataset = load_from_disk('open_i_raw')\n",
    "def normalize_text_samples(sample: dict) -> dict:\n",
    "  return {\n",
    "      'findings_normalized': normalize_text(sample['findings']),\n",
    "      'impression_normalized': normalize_text(sample['impression'])\n",
    "  }\n",
    "data = data.map(normalize_text_samples)\n",
    "\n",
    "print(data) # Overview over the dataset\n",
    "print(data[0])  # The first sample of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSXv8QNeAn-b"
   },
   "source": [
    "3. Apply split_sentences to the findings and impression sections using the map function of datasets.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EY4-GRj3YiZb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3955/3955 [00:00<00:00, 21921.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['uId', 'findings', 'impression', 'findings_normalized', 'impression_normalized', 'findings_sentences', 'impression_sentences'],\n",
      "    num_rows: 3955\n",
      "})\n",
      "{'uId': 'CXR162', 'findings': 'Heart size normal. Lungs are clear. XXXX are normal. No pneumonia, effusions, edema, pneumothorax, adenopathy, nodules or masses.', 'impression': 'Normal chest', 'findings_normalized': None, 'impression_normalized': None, 'findings_sentences': None, 'impression_sentences': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3955/3955 [00:00<00:00, 830544.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def split_sample_sentences(sample: dict) -> dict:\n",
    "  return {\n",
    "      'findings_sentences': split_sentences(sample['findings']),\n",
    "      'impression_sentences': split_sentences(sample['impression'])\n",
    "  }\n",
    "\n",
    "split_dataset = data.map(split_sample_sentences)\n",
    "\n",
    "print(split_dataset)\n",
    "print(split_dataset[0])\n",
    "split_dataset.save_to_disk('open_i_split')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GA3p83wOwdy8"
   },
   "source": [
    "## Step 4 - Vocabulary Creation: Train a tokenizer\n",
    "In this step the goal is to learn a vocabulary (i.e. tokenizer) from the Open-I sentences. We therefore extract a list of all sentences (from findings and impression section) in the dataset. This list is then used for learning the vocabulary.\n",
    "\n",
    "TODO: Extract all sentences and store them in all_sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UNp27c98Fs4V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['uId', 'findings', 'impression', 'findings_normalized', 'impression_normalized', 'findings_sentences', 'impression_sentences'],\n",
      "    num_rows: 3955\n",
      "})\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# TODO: extract the list of all sentences in the whole dataset (from both sections)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m all_sentences: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal number of sentences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_sentences)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal number of words (by whitespaces): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sent\u001b[38;5;241m.\u001b[39msplit())\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39msent\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mall_sentences)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m num_unique_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(word \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m all_sentences \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent\u001b[38;5;241m.\u001b[39msplit()))\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "split_dataset: Dataset = load_from_disk('open_i_split')\n",
    "print(split_dataset)\n",
    "\n",
    "# TODO: extract the list of all sentences in the whole dataset (from both sections)\n",
    "all_sentences: List[str] = None\n",
    "\n",
    "print(f'\\nTotal number of sentences: {len(all_sentences)}')\n",
    "print(f'Total number of words (by whitespaces): {sum(len(sent.split()) for sent in all_sentences)}')\n",
    "num_unique_words = len(set(word for sent in all_sentences for word in sent.split()))\n",
    "print(f'Total number of different words: {num_unique_words}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOa8C0_4WdOi"
   },
   "source": [
    "We no define the initial alphabet (optional, by default the alphabet is derived from the characters present in the dataset) and the size of the vocabulary we want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8uzL_qe38is"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# All ascii characters (lowercase only) + digits\n",
    "initial_alphabet = list(string.ascii_lowercase) + list(string.digits)\n",
    "print(f'Initial alphabet size: {len(initial_alphabet)}')\n",
    "\n",
    "vocab_size = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfi-3JroWpCb"
   },
   "source": [
    "Now we use the huggingface tokenizer library to learn a WordPiece tokenizer. This is already implemented but you can try with different arguments or also try other tokenizers, e.g. BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmWoQGHy3da2"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers, processors\n",
    "\n",
    "special_tokens = [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"]\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token='<UNK>'))\n",
    "# we use a lowercase vocabulary\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"<BOS> $0 <EOS>\",\n",
    "    pair=\"<BOS> $A <EOS> <BOS> $B <EOS>\",\n",
    "    special_tokens=[(\"<BOS>\", 2), (\"<EOS>\", 3)],\n",
    ")\n",
    "tokenizer.decoder = decoders.WordPiece()\n",
    "\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    initial_alphabet=initial_alphabet,\n",
    "    special_tokens=special_tokens,\n",
    ")\n",
    "\n",
    "# now train it\n",
    "tokenizer.train_from_iterator(all_sentences, trainer=trainer)\n",
    "\n",
    "print(f'Vocab size: {tokenizer.get_vocab_size()}')\n",
    "print('Vocab: \\n', tokenizer.get_vocab())\n",
    "\n",
    "tokenizer.save('tokenizer.json', pretty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5O4vgqeFtTN"
   },
   "source": [
    "### Inspection of the created vocabulary\n",
    "You can now take a look at the created vocabulary by opening the file \"tokenizer.json\".\n",
    "\n",
    "You can also try around tokenizing some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YYXc8_t8mvF"
   },
   "outputs": [],
   "source": [
    "example_section = split_dataset[0]['findings_sentences']\n",
    "example_sentence = example_section[0]\n",
    "\n",
    "print(example_sentence)\n",
    "\n",
    "encoded = tokenizer.encode(example_sentence)\n",
    "print(f'Encoded tokens: {encoded.tokens}')\n",
    "print(f'Encoded token ids: {encoded.ids}')\n",
    "print(f'Decoded again: {tokenizer.decode(encoded.ids)}')\n",
    "print(f'Decoded (keep special tokens): {tokenizer.decode(encoded.ids, skip_special_tokens=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBPyy6E5wpVC"
   },
   "source": [
    "# Task 2: Training a Text Classifier (graded!)\n",
    "## Goals\n",
    "- Understand how pre-trained language models can be utilized for downstream tasks\n",
    "\n",
    "## Tools\n",
    "- [Huggingface Transformers](https://huggingface.co/transformers/): Library for pre-trained transformer-based language models\n",
    "\n",
    "## Dataset\n",
    "Medical Transcriptions [[Kaggle](https://www.kaggle.com/tboyle10/medicaltranscriptions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9olvk3WNS5Uc"
   },
   "source": [
    "## Step 1 - Load the Medical Transactions Dataset (Nothing TODO here)\n",
    "We first load the Medical Transcriptions dataset as a huggingface dataset. The dataset is split into train and test set, so the returned type is a datasets.DatasetDict, which acts like a dictionary of datasets.Dataset but provides utility functions, e.g. for mapping all datasets of the dict using the map method.\n",
    "This step is already implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e05T0u7US1jt"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_transciptions_dataset() -> Tuple[DatasetDict, List[str]]:\n",
    "    df = pd.read_csv('medical_transcriptions/mtsamples.csv')\n",
    "    df = df.drop(['Unnamed: 0'],axis=1,)\n",
    "\n",
    "    counts = df['medical_specialty'].value_counts()\n",
    "    print(f'Original counts:\\n{counts}')\n",
    "    dropped_specialities = [k for k, v in counts.items() if v < 100]\n",
    "    for dropped_speciality in dropped_specialities:\n",
    "      df = df[df['medical_specialty'] != dropped_speciality]\n",
    "    df.dropna(inplace=True)\n",
    "    counts = df['medical_specialty'].value_counts()\n",
    "    print(f'Counts after removing small specialities:\\n{counts}')\n",
    "\n",
    "    df['medical_specialty'] = df['medical_specialty'].astype('category')\n",
    "    class_names = df['medical_specialty'].cat.categories.tolist()\n",
    "    print(f'Class names : {class_names}')\n",
    "    df['medical_specialty'] = df['medical_specialty'].cat.codes\n",
    "\n",
    "    train, test = train_test_split(df, stratify=df['medical_specialty'], test_size=0.25)\n",
    "    dataset = DatasetDict({'train': Dataset.from_pandas(train), 'test': Dataset.from_pandas(test)})\n",
    "    dataset = dataset.remove_columns(['__index_level_0__'])\n",
    "\n",
    "    return dataset, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKtCzBSp1ADK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original counts:\n",
      " Surgery                          1103\n",
      " Consult - History and Phy.        516\n",
      " Cardiovascular / Pulmonary        372\n",
      " Orthopedic                        355\n",
      " Radiology                         273\n",
      " General Medicine                  259\n",
      " Gastroenterology                  230\n",
      " Neurology                         223\n",
      " SOAP / Chart / Progress Notes     166\n",
      " Obstetrics / Gynecology           160\n",
      " Urology                           158\n",
      " Discharge Summary                 108\n",
      " ENT - Otolaryngology               98\n",
      " Neurosurgery                       94\n",
      " Hematology - Oncology              90\n",
      " Ophthalmology                      83\n",
      " Nephrology                         81\n",
      " Emergency Room Reports             75\n",
      " Pediatrics - Neonatal              70\n",
      " Pain Management                    62\n",
      " Psychiatry / Psychology            53\n",
      " Office Notes                       51\n",
      " Podiatry                           47\n",
      " Dermatology                        29\n",
      " Cosmetic / Plastic Surgery         27\n",
      " Dentistry                          27\n",
      " Letters                            23\n",
      " Physical Medicine - Rehab          21\n",
      " Sleep Medicine                     20\n",
      " Endocrinology                      19\n",
      " Bariatrics                         18\n",
      " IME-QME-Work Comp etc.             16\n",
      " Chiropractic                       14\n",
      " Rheumatology                       10\n",
      " Diets and Nutritions               10\n",
      " Speech - Language                   9\n",
      " Autopsy                             8\n",
      " Lab Medicine - Pathology            8\n",
      " Allergy / Immunology                7\n",
      " Hospice - Palliative Care           6\n",
      "Name: medical_specialty, dtype: int64\n",
      "Counts after removing small specialities:\n",
      " Surgery                          1021\n",
      " Orthopedic                        303\n",
      " Cardiovascular / Pulmonary        280\n",
      " Radiology                         251\n",
      " Consult - History and Phy.        234\n",
      " Gastroenterology                  195\n",
      " Neurology                         168\n",
      " General Medicine                  146\n",
      " SOAP / Chart / Progress Notes     142\n",
      " Urology                           140\n",
      " Obstetrics / Gynecology           130\n",
      " Discharge Summary                  77\n",
      "Name: medical_specialty, dtype: int64\n",
      "Class names : [' Cardiovascular / Pulmonary', ' Consult - History and Phy.', ' Discharge Summary', ' Gastroenterology', ' General Medicine', ' Neurology', ' Obstetrics / Gynecology', ' Orthopedic', ' Radiology', ' SOAP / Chart / Progress Notes', ' Surgery', ' Urology']\n"
     ]
    }
   ],
   "source": [
    "dataset, class_names = load_transciptions_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMwK0iiUvhUV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  2315\n",
      "Test samples:  772\n",
      "Columns:  {'description': Value(dtype='string', id=None), 'medical_specialty': Value(dtype='int8', id=None), 'sample_name': Value(dtype='string', id=None), 'transcription': Value(dtype='string', id=None), 'keywords': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print('Training samples: ', len(dataset['train']['transcription']))\n",
    "print('Test samples: ', len(dataset['test']['transcription']))\n",
    "print('Columns: ', dataset['train'].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKthj9SypvMB"
   },
   "source": [
    "## Step 2 - Compute sentence embeddings using pre-trained language model\n",
    "Our goal is to use a pre-trained language model (from the huggingface transformers library) to encode sentences into sentence representations. Therefore, each sentence is tokenized and passed to the language model. The resulting contextualized token representations (i.e. the outputs of the last hidden layer of the language model) are then globally pooled to get sentence-level representations. As we use a pre-trained model, no training is involved in this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTggsRsJHxCM"
   },
   "source": [
    "### Specify pre-trained language model\n",
    "TODO: Decide which pre-trained language model you want to use and specify its name here.\n",
    "You can search for models at the huggingface model hub: https://huggingface.co/models . You can either use standard models, e.g. BERT, or use biomedcial models.\n",
    "\n",
    "When you decided for a model, copy the name of the model from the URL (removing only https://huggingface.co/) and insert it as model_name here.\n",
    "For more reference on how to use the tokenizer have a look at https://huggingface.co/docs/transformers/preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1h6SqqcHwIZ"
   },
   "outputs": [],
   "source": [
    "# TODO: Insert model name here\n",
    "model_name = 'google-bert/bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k85a1cxn1RSG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer (nothing TODO)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSaLG4Y6H113"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valentinbiller/anaconda3/lib/python3.9/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the model (nothing TODO) and inspect it\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-U1wMp2SHInc"
   },
   "source": [
    "### Experiment with tokenizer and language model (nothing to implement here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aipe5EhsTaIf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example text:  ['PREOPERATIVE DIAGNOSES:,1.  Status post multiple trauma/motor vehicle accident.,2.  Acute respiratory failure.,3.  Acute respiratory distress/ventilator asynchrony.,4.  Hypoxemia.,5.  Complete atelectasis of left lung.,POSTOPERATIVE DIAGNOSES:,1.  Status post multiple trauma/motor vehicle accident.,2.  Acute respiratory failure.,3.  Acute respiratory distress/ventilator asynchrony.,4.  Hypoxemia.,5.  Complete atelectasis of left lung.,6.  Clots partially obstructing the endotracheal tube and completely obstructing the entire left main stem and entire left bronchial system.,PROCEDURE PERFORMED: ,Emergent fiberoptic plus bronchoscopy with lavage.,LOCATION OF PROCEDURE:  ,ICU.  Room #164.,ANESTHESIA/SEDATION:,  Propofol drip, Brevital 75 mg, morphine 5 mg, and Versed 8 mg.,HISTORY,:  The patient is a 44-year-old male who was admitted to ABCD Hospital on 09/04/03 status post MVA with multiple trauma and subsequently diagnosed with multiple spine fractures as well as bilateral pulmonary contusions, requiring ventilatory assistance.  The patient was noted with acute respiratory distress on ventilator support with both ventilator asynchrony and progressive desaturation.  Chest x-ray as noted above revealed complete atelectasis of the left lung.  The patient was subsequently sedated and received one dose of paralytic as noted above followed by emergent fiberoptic flexible bronchoscopy.,PROCEDURE DETAIL,:  A bronchoscope was inserted through the oroendotracheal tube, which was partially obstructed with blood clots.  These were lavaged with several aliquots of normal saline until cleared.  The bronchoscope required removal because the tissue/clots were obstructing the bronchoscope.  The bronchoscope was reinserted on several occasions until cleared and advanced to the main carina.  The endotracheal tube was noted to be in good position.  The bronchoscope was advanced through the distal trachea.  There was a white tissue completely obstructing the left main stem at the carina.  The bronchoscope was advanced to this region and several aliquots of normal saline lavage were instilled and suctioned.  Again this partially obstructed the bronchoscope requiring several times removing the bronchoscope to clear the lumen.  The bronchoscope subsequently was advanced into the left mainstem and subsequently left upper and lower lobes.  There was diffuse mucus impactions/tissue as well as intermittent clots.  There was no evidence of any active bleeding noted.  Bronchoscope was adjusted and the left lung lavaged until no evidence of any endobronchial obstruction is noted.  Bronchoscope was then withdrawn to the main carina and advanced into the right bronchial system.  There is no plugging or obstruction of the right bronchial system.  The bronchoscope was then withdrawn to the main carina and slowly withdrawn as the position of endotracheal tube was verified, approximately 4 cm above the main carina.  The bronchoscope was then completely withdrawn as the patient was maintained on ventilator support during and postprocedure.  Throughout the procedure, pulse oximetry was greater than 95% throughout.  There is no hemodynamic instability or variability noted during the procedure.  Postprocedure chest x-ray is pending at this time.', \"PROCEDURE:,  Gastroscopy.,PREOPERATIVE DIAGNOSIS:,  Dysphagia and globus.,POSTOPERATIVE DIAGNOSIS: , Normal.,MEDICATIONS:,  MAC.,DESCRIPTION OF PROCEDURE: , The Olympus gastroscope was introduced through the oropharynx and passed carefully through the esophagus and stomach, and then through the gastrojejunal anastomosis into the efferent jejunal loop.  The preparation was good and all surfaces were well seen.  The hypopharynx was normal with no evidence of inflammation.  The esophagus had a normal contour and normal mucosa throughout with no sign of stricturing or inflammation or exudate.  The GE junction was located at 39 cm from the incisors and appeared normal with no evidence of reflux, damage, or Barrett's.  Below this there was a small gastric pouch measuring 6 cm with intact mucosa and no retained food.  The gastrojejunal anastomosis was patent measuring about 12 mm, with no inflammation or ulceration.  Beyond this there was a side-to-side gastrojejunal anastomosis with a short afferent blind end and a normal efferent end with no sign of obstruction or inflammation.  The scope was withdrawn and the patient was sent to recovery room.  She tolerated the procedure well.,FINAL DIAGNOSES:,1.  Normal post-gastric bypass anatomy.,2.  No evidence of inflammation or narrowing to explain her symptoms.\"]\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "input ids:  [[101, 3653, 25918, 8082, 22939, 26745, 8583, 1024, 1010, 1015, 1012, 3570, 2695, 3674, 12603, 1013, 5013, 4316, 4926, 1012, 1010, 1016, 1012, 11325, 16464, 4945, 1012, 1010, 1017, 1012, 11325, 16464, 12893, 1013, 18834, 11733, 4263, 2004, 6038, 2818, 4948, 2100, 1012, 1010, 1018, 1012, 1044, 22571, 11636, 17577, 1012, 1010, 1019, 1012, 3143, 8823, 2571, 25572, 6190, 1997, 2187, 11192, 1012, 1010, 2695, 25918, 8082, 22939, 26745, 8583, 1024, 1010, 1015, 1012, 3570, 2695, 3674, 12603, 1013, 5013, 4316, 4926, 1012, 1010, 1016, 1012, 11325, 16464, 4945, 1012, 1010, 1017, 1012, 11325, 16464, 12893, 1013, 18834, 11733, 4263, 2004, 6038, 2818, 4948, 2100, 1012, 1010, 1018, 1012, 1044, 22571, 11636, 17577, 1012, 1010, 1019, 1012, 3143, 8823, 2571, 25572, 6190, 1997, 2187, 11192, 1012, 1010, 1020, 1012, 18856, 12868, 6822, 27885, 3367, 6820, 11873, 1996, 2203, 4140, 22648, 20192, 2140, 7270, 1998, 3294, 27885, 3367, 6820, 11873, 1996, 2972, 2187, 2364, 7872, 1998, 2972, 2187, 22953, 12680, 4818, 2291, 1012, 1010, 7709, 2864, 1024, 1010, 12636, 3372, 11917, 7361, 4588, 4606, 22953, 12680, 2891, 3597, 7685, 2007, 13697, 3351, 1012, 1010, 3295, 1997, 7709, 1024, 1010, 24582, 2226, 1012, 2282, 1001, 17943, 1012, 1010, 2019, 25344, 1013, 7367, 20207, 1024, 1010, 17678, 11253, 4747, 27304, 1010, 7987, 6777, 18400, 4293, 11460, 1010, 22822, 20738, 1019, 11460, 1010, 1998, 7893, 2094, 1022, 11460, 1012, 1010, 2381, 1010, 1024, 1996, 5776, 2003, 1037, 4008, 1011, 2095, 1011, 2214, 3287, 2040, 2001, 4914, 2000, 5925, 2094, 2902, 2006, 5641, 1013, 5840, 1013, 6021, 3570, 2695, 19842, 2050, 2007, 3674, 12603, 1998, 3525, 11441, 2007, 3674, 8560, 28929, 2004, 2092, 2004, 17758, 21908, 9530, 5809, 8496, 1010, 9034, 18834, 11733, 7062, 5375, 1012, 1996, 5776, 2001, 3264, 2007, 11325, 16464, 12893, 2006, 18834, 11733, 4263, 2490, 2007, 2119, 18834, 11733, 4263, 2004, 6038, 2818, 4948, 2100, 1998, 6555, 4078, 4017, 18924, 1012, 3108, 1060, 1011, 4097, 2004, 3264, 2682, 3936, 3143, 8823, 2571, 25572, 6190, 1997, 1996, 2187, 11192, 1012, 1996, 5776, 2001, 3525, 7367, 13701, 2094, 1998, 2363, 2028, 13004, 1997, 11498, 2135, 4588, 2004, 3264, 2682, 2628, 2011, 12636, 3372, 11917, 7361, 4588, 12379, 22953, 12680, 2891, 3597, 7685, 1012, 1010, 7709, 6987, 1010, 1024, 1037, 22953, 12680, 2891, 16186, 2001, 12889, 2083, 1996, 20298, 10497, 4140, 22648, 20192, 2140, 7270, 1010, 2029, 2001, 6822, 27885, 3367, 6820, 10985, 2007, 2668, 18856, 12868, 1012, 2122, 2020, 13697, 5999, 2007, 2195, 4862, 28940, 12868, 1997, 3671, 28413, 2127, 5985, 1012, 1996, 22953, 12680, 2891, 16186, 3223, 8208, 2138, 1996, 8153, 1013, 18856, 12868, 2020, 27885, 3367, 6820, 11873, 1996, 22953, 12680, 2891, 16186, 1012, 1996, 22953, 12680, 2891, 16186, 2001, 19222, 28728, 2006, 2195, 6642, 2127, 5985, 1998, 3935, 2000, 1996, 2364, 2482, 3981, 1012, 1996, 2203, 4140, 22648, 20192, 2140, 7270, 2001, 3264, 2000, 2022, 1999, 2204, 2597, 1012, 1996, 22953, 12680, 2891, 16186, 2001, 3935, 2083, 1996, 29333, 19817, 15395, 2050, 1012, 2045, 2001, 1037, 2317, 8153, 3294, 27885, 3367, 6820, 11873, 1996, 2187, 2364, 7872, 2012, 1996, 2482, 3981, 1012, 1996, 22953, 12680, 2891, 16186, 2001, 3935, 2000, 2023, 102], [101, 7709, 1024, 1010, 3806, 13181, 9363, 7685, 1012, 1010, 3653, 25918, 8082, 11616, 1024, 1010, 1040, 7274, 21890, 10440, 1998, 1043, 4135, 8286, 1012, 1010, 2695, 25918, 8082, 11616, 1024, 1010, 3671, 1012, 1010, 20992, 1024, 1010, 6097, 1012, 1010, 6412, 1997, 7709, 1024, 1010, 1996, 26742, 3806, 13181, 26127, 2001, 3107, 2083, 1996, 20298, 21890, 18143, 2595, 1998, 2979, 5362, 2083, 1996, 9686, 7361, 3270, 12349, 1998, 4308, 1010, 1998, 2059, 2083, 1996, 3806, 13181, 6460, 19792, 2389, 9617, 16033, 15530, 2483, 2046, 1996, 1041, 12494, 4765, 15333, 19792, 2389, 7077, 1012, 1996, 7547, 2001, 2204, 1998, 2035, 9972, 2020, 2092, 2464, 1012, 1996, 1044, 22571, 7361, 8167, 6038, 2595, 2001, 3671, 2007, 2053, 3350, 1997, 21733, 1012, 1996, 9686, 7361, 3270, 12349, 2018, 1037, 3671, 9530, 21163, 1998, 3671, 14163, 13186, 2050, 2802, 2007, 2053, 3696, 1997, 9384, 12228, 2030, 21733, 2030, 4654, 14066, 2618, 1012, 1996, 16216, 5098, 2001, 2284, 2012, 4464, 4642, 2013, 1996, 4297, 19565, 2869, 1998, 2596, 3671, 2007, 2053, 3350, 1997, 25416, 25148, 1010, 4053, 1010, 2030, 12712, 1005, 1055, 1012, 2917, 2023, 2045, 2001, 1037, 2235, 3806, 12412, 21445, 9854, 1020, 4642, 2007, 10109, 14163, 13186, 2050, 1998, 2053, 6025, 2833, 1012, 1996, 3806, 13181, 6460, 19792, 2389, 9617, 16033, 15530, 2483, 2001, 7353, 9854, 2055, 2260, 3461, 1010, 2007, 2053, 21733, 2030, 17359, 19357, 3508, 1012, 3458, 2023, 2045, 2001, 1037, 2217, 1011, 2000, 1011, 2217, 3806, 13181, 6460, 19792, 2389, 9617, 16033, 15530, 2483, 2007, 1037, 2460, 21358, 7512, 4765, 6397, 2203, 1998, 1037, 3671, 1041, 12494, 4765, 2203, 2007, 2053, 3696, 1997, 27208, 2030, 21733, 1012, 1996, 9531, 2001, 9633, 1998, 1996, 5776, 2001, 2741, 2000, 7233, 2282, 1012, 2016, 25775, 1996, 7709, 2092, 1012, 1010, 2345, 22939, 26745, 8583, 1024, 1010, 1015, 1012, 3671, 2695, 1011, 3806, 12412, 11826, 13336, 1012, 1010, 1016, 1012, 2053, 3350, 1997, 21733, 2030, 21978, 2000, 4863, 2014, 8030, 1012, 102]]\n",
      "attention mask:  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "length:  512\n"
     ]
    }
   ],
   "source": [
    "# Tokenize some example text and inspect the results to understand the outputs of the tokenizer\n",
    "sample_text = dataset['train'][:2]['transcription']  # batch of 2 samples of text\n",
    "print('example text: ', sample_text)\n",
    "toknized = tokenizer(sample_text, truncation=True, max_length=512)\n",
    "print(toknized.keys())\n",
    "print('input ids: ', toknized['input_ids'])\n",
    "print('attention mask: ', toknized['attention_mask'])\n",
    "print('length: ', len(toknized['input_ids'][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XK7Zzjyp3yJ7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "input ids:  tensor([[  101,  3653, 25918,  ...,  2000,  2023,   102],\n",
      "        [  101,  7709,  1024,  ...,     0,     0,     0]])\n",
      "attention mask:  tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "shape:  torch.Size([2, 512])\n",
      "output shape:  torch.Size([2, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "# Feed some example data to understand the outputs of the language model\n",
    "\n",
    "# pad the batch and convert it to Pytorch\n",
    "# return_tensors='pt' => return the tokenized values as PyTorch tensors instead of lists\n",
    "x = tokenizer.pad(toknized, return_tensors='pt')\n",
    "print(x.keys())\n",
    "print('input ids: ', x['input_ids'])\n",
    "print('attention mask: ', x['attention_mask'])\n",
    "print('shape: ', x['input_ids'].shape)\n",
    "\n",
    "# Encode the tokenized and padded input\n",
    "results = model(**x)\n",
    "print('output shape: ', results.last_hidden_state.shape)  # (batch_size x num_tokens x d_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrqTaJHVEF0r"
   },
   "source": [
    "### Tokenize the dataset\n",
    "We now tokenize the whole dataset using the batched map-method.\n",
    "\n",
    "TODO: Implement the tokenize_batch function using the tokenizer. Note: do not pad the data yet but truncate it to a max length of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MN1mFK1uEHeE"
   },
   "outputs": [],
   "source": [
    "def tokenize_batch(text_batch: List[str]):\n",
    "  # TODO: implement tokenize batch using the tokenizer\n",
    "  return tokenizer(text_batch, truncation=True, max_length=512, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9VfStgkEip8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2315 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/772 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after tokenization ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenize_batch to dataset (nothing TODO here)\n",
    "tokenized_dataset = dataset.map(lambda examples: tokenize_batch(examples[\"transcription\"]), batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['sample_name', 'transcription', 'description', 'keywords']).rename_column('medical_specialty', 'labels')\n",
    "print('Columns after tokenization', list(tokenized_dataset['train'].features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVq84WgQG9BR"
   },
   "source": [
    "### Token Pooling\n",
    "Define Pooling functions to compute sentence embeddings from token outputs.\n",
    "\n",
    "TODO: implement three possible pooling functions\n",
    "- CLS: use the output of the [CLS] token only and ignore the other tokens.\n",
    "- max/avg: globally max/avg pool over all token outputs, but make sure to ignore padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQzwuFxMGax9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def CLS_pool(hidden_state: torch.FloatTensor, attention_mask: torch.BoolTensor):\n",
    "    \"\"\"\n",
    "    Returns only the hidden state of the [CLS] token\n",
    "    :param hidden_state: (N x M x d_hidden) where N is the batch size and M is the number of tokens\n",
    "    :param attention_mask: (N x M), True for \"real\" tokens, False for padding tokens.\n",
    "    :return (N x d_hidden)\n",
    "    \"\"\"\n",
    "    # TODO: implement function\n",
    "    return hidden_state[:, 0, :]\n",
    "\n",
    "def max_pool(hidden_state: torch.FloatTensor, attention_mask: torch.BoolTensor):\n",
    "    \"\"\"\n",
    "    Globally pools the hidden states over all tokens using max pooling.\n",
    "    :param hidden_state: (N x M x d_hidden) where N is the batch size and M is the number of tokens\n",
    "    :param attention_mask: (N x M), True for \"real\" tokens, False for padding tokens.\n",
    "    :return (N x d_hidden)\n",
    "    \"\"\"\n",
    "    # TODO: implement function\n",
    "    mask_expanded = attention_mask.bool().unsqueeze(-1).expand_as(hidden_state)\n",
    "    hidden_state_masked = torch.where(mask_expanded, hidden_state, torch.tensor(-1e9).to(hidden_state.dtype).to(hidden_state.device))\n",
    "\n",
    "    max_pooled, _ = torch.max(hidden_state_masked, dim=1)\n",
    "    return max_pooled\n",
    "\n",
    "def avg_pool(hidden_state: torch.FloatTensor, attention_mask: torch.BoolTensor):\n",
    "    \"\"\"\n",
    "    Globally pools the hidden states over all tokens using average pooling.\n",
    "    :param hidden_state: (N x M x d_hidden) where N is the batch size and M is the number of tokens\n",
    "    :param attention_mask: (N x M), True for \"real\" tokens, False for padding tokens.\n",
    "    :return (N x d_hidden)\n",
    "    \"\"\"\n",
    "    # TODO: implement function\n",
    "    mask_expanded = attention_mask.bool().unsqueeze(-1).expand_as(hidden_state).float()\n",
    "    sum_masks = mask_expanded.sum(1)  # (N x 1 x d_hidden)\n",
    "    \n",
    "    sum_pooled = torch.sum(hidden_state * mask_expanded, dim=1)\n",
    "    avg_pooled = sum_pooled / sum_masks.clamp(min=1e-9)\n",
    "\n",
    "    return avg_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7jf0k3_H5nQ"
   },
   "source": [
    "### Sentence Embedder\n",
    "The sentence embedder takes a tokenized input, uses the language model to compute token representations (i.e. the last hidden_state of the language model) and then uses a pooling function to compute a single sentence representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUZs8TqU2Gnl"
   },
   "source": [
    "TODO: implement the sentence embedder forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLyFxgkTIA5h"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from typing import Dict\n",
    "\n",
    "class SentenceEmbedder(nn.Module):\n",
    "  def __init__(self, model, pool):\n",
    "    super().__init__()\n",
    "    self.model = model  # this is a huggingface language model from AutoModel.from_pretrained\n",
    "    self.pool = pool  # this is one of the three defined pooling functions (CLS, max, avg)\n",
    "\n",
    "  def forward(self, x: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :param x: dict containing the following elements:\n",
    "      - input_ids: torch.Tensor of the token_type ids with shape (N x M)\n",
    "      - attention_mask: torch.Tensor containing the attention mask of shape (N x M)\n",
    "    :return sentence_embedding for each sample of shape (N x d_hidden)\n",
    "    \"\"\"\n",
    "    # TODO: implement forward\n",
    "\n",
    "    outputs = self.model(input_ids=x['input_ids'], attention_mask=x['attention_mask'])\n",
    "    \n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    \n",
    "    sentence_embedding = self.pool(last_hidden_state, x['attention_mask'])\n",
    "\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAIiUT9D2Iu9"
   },
   "source": [
    "TODO: now decide which pooling function you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6J8KflrZFAj"
   },
   "outputs": [],
   "source": [
    "# TODO: try different pooling functions (nothing more to implement here)\n",
    "# (also change the dataset_name so you can later easily switch between pooling functions without the need to recompute the embeddings)\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    \n",
    "# device = 'cpu'\n",
    "\n",
    "dataset_name = 'encoded_transciptions_AVG'\n",
    "pool = avg_pool\n",
    "\n",
    "sentence_embedder = SentenceEmbedder(model, pool).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miiKNXPV2OS8"
   },
   "source": [
    " Now run the sentence embedder (nothing TODO here).\n",
    "\n",
    " The resulting dataset will then contain a sentence_embedding column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-q6nYkT0VkX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function embed_sentence at 0x16a136790> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2315 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/772 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2315 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/772 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def embed_sentence(batch):\n",
    "  model_input = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}\n",
    "  model_input = tokenizer.pad(model_input, return_tensors='pt').to(device=device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    sentence_embeddings = sentence_embedder(model_input)\n",
    "  return {'sentence_embedding': sentence_embeddings.detach().cpu().numpy()}\n",
    "\n",
    "encoded_dataset = tokenized_dataset.map(embed_sentence, batched=True, batch_size=64)\n",
    "encoded_dataset.save_to_disk(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QzqwS5rJCK9"
   },
   "source": [
    "## Step 5 - Train Classification Model on Sentence Embeddings\n",
    "Now we have a single sentence representation vector for each sentence. We now learn a simple classifier based on a MLP (i.e. a 2-layer fully-connected neural network). We first project each sentence embedding from d_hidden to d_mlp, apply ReLU (or another non-linearity) and then project the resulting vector into num_classes where we then apply the cross entropy loss. The classification head will then be learned based on the training data. This is the only learned component in our model.\n",
    "\n",
    "TODO: implement the MLP head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNnx-9QzsIng"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import datasets\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "  def __init__(self, d_hidden: int, d_mlp: int, num_classes: int):\n",
    "    super().__init__()\n",
    "    # TODO: add the required layers here\n",
    "    self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(d_hidden, d_mlp),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_mlp, num_classes)\n",
    "    )\n",
    "\n",
    "    # TODO: define the loss function to use here\n",
    "    self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "  def forward(self, x, y_true):\n",
    "    \"\"\"\n",
    "    :param x: sentence embeddings (N x d_hidden)\n",
    "    :param y_true: target classes (multiclass) (N)\n",
    "    \"\"\"\n",
    "    # TODO: apply MLP head to sentence embeddings\n",
    "    # (N x num_classes)\n",
    "    logits = self.mlp_head(x)\n",
    "\n",
    "    # TODO: apply the loss function\n",
    "    loss = self.loss(logits, y_true) if y_true is not None else None\n",
    "    # TODO: compute the predictions (i.e. target classes as longs) from the logits\n",
    "    # (N)\n",
    "    _, y_pred = torch.max(logits, dim=1)\n",
    "\n",
    "    return y_pred, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTTplEVL3g_g"
   },
   "source": [
    "Now train the classification head on the sentence embeddings.\n",
    "The training is already implemented.\n",
    "\n",
    "TODO: specify and try different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZK_cuGPn1jUZ"
   },
   "outputs": [],
   "source": [
    "# TODO: try different hyperparameters\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "dataset_name = 'encoded_transciptions_AVG'\n",
    "num_epochs = 20\n",
    "lr = 5e-4\n",
    "weight_decay = 1e-4\n",
    "d_hidden = 768  # must match the language model hidden output\n",
    "d_mlp = 1024\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MXXBwIyqAjU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classes:  12\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 19/19 [00:00<00:00, 38.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  2.1185073852539062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 195.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.7768936157226562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 201.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.593662977218628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 173.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.5410226583480835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 177.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.4411345720291138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 175.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.4048923254013062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 172.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.3656760454177856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 178.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.3311318159103394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 174.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.2859617471694946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 171.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.2920438051223755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 146.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.2504011392593384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 147.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.2291159629821777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 145.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.19967520236969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 145.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.19120454788208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 144.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.1519535779953003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 130.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.1699175834655762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 121.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.1350510120391846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 113.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.145141363143921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 108.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.1350880861282349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 103.13it/s]\n",
      "/var/folders/lb/3yybq3q15h95_b40m72w81vc0000gn/T/ipykernel_56845/1963609609.py:39: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  f1_metric = datasets.load_metric(\"f1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.0997015237808228\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 7/7 [00:00<00:00, 106.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test loss:  1.28374183177948\n",
      "F1 (Macro):  {'f1': 0.3769100747896881}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now run the training...\n",
    "\n",
    "print('num classes: ', len(class_names))\n",
    "encoded_dataset = load_from_disk(dataset_name)\n",
    "# make sure PyTorch Tensors are returned\n",
    "encoded_dataset.set_format('pt')\n",
    "\n",
    "# remove all columns except sentence_embedding and labels\n",
    "columns_to_remove = set(encoded_dataset['train'].column_names) - {'sentence_embedding', 'labels'}\n",
    "encoded_dataset = encoded_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "train_data_loader = DataLoader(encoded_dataset['train'], batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(encoded_dataset['test'], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classification_model = ClassificationHead(d_hidden=d_hidden,\n",
    "                                          d_mlp=d_mlp,\n",
    "                                          num_classes=len(class_names))\n",
    "classification_model = classification_model.to(device=device)\n",
    "optimizer = torch.optim.Adam(classification_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "classification_model.train()\n",
    "print('Training...')\n",
    "for epoch in range(num_epochs):\n",
    "  train_loss = []\n",
    "  for train_batch in tqdm(train_data_loader):\n",
    "    x = train_batch['sentence_embedding'].to(device=device)\n",
    "    y_true = train_batch['labels'].to(device=device)\n",
    "\n",
    "    y_pred, loss = classification_model(x, y_true)\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "  print('train loss: ', torch.mean(torch.tensor(train_loss)).item())\n",
    "\n",
    "classification_model.eval()\n",
    "print('Testing...')\n",
    "f1_metric = datasets.load_metric(\"f1\")\n",
    "with torch.no_grad():\n",
    "  test_loss = []\n",
    "  for test_batch in tqdm(test_data_loader):\n",
    "    x = test_batch['sentence_embedding'].to(device=device)\n",
    "    y_true = test_batch['labels'].to(device=device)\n",
    "\n",
    "    y_pred, loss = classification_model(x, y_true)\n",
    "\n",
    "    f1_metric.add_batch(predictions=y_pred, references=y_true)\n",
    "    test_loss.append(loss)\n",
    "print('\\ntest loss: ', torch.mean(torch.tensor(test_loss)).item())\n",
    "print('F1 (Macro): ', f1_metric.compute(average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyGeE0JKWmJN"
   },
   "source": [
    "# Task 3: Generating text from a pre-trained decoder LM (graded!)\n",
    "## Goals\n",
    "- Understand how inference (text generation) works on decoder languages models\n",
    "\n",
    "## Tools\n",
    "- [Huggingface Transformers](https://huggingface.co/transformers/): Library for pre-trained transformer-based language models)\n",
    "\n",
    "## Model\n",
    "- Architecture: https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel\n",
    "- Model weights: https://huggingface.co/healx/gpt-2-pubmed-medium\n",
    "\n",
    "## Notes and Tips\n",
    "- You can read this blog post to learn more about generation strategies: https://huggingface.co/blog/how-to-generate\n",
    "- We are going to implement greedy-search.\n",
    "- Please do not use the \"generate\" method of the pre-trained model (as there generation is already implemented)\n",
    "- You can however take a look at their implementation of greedy-search: https://github.com/huggingface/transformers/blob/04ab5605fbb4ef207b10bf2772d88c53fc242e83/src/transformers/generation/utils.py#L2080\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlHUnMYEY49m"
   },
   "source": [
    "## Step 1: Implement generate method\n",
    "Implement the generate method that takes a language model and its tokenizer together with a list of text prefixes and outputs a list of generated sentences (one for each prefix). The prefixes should be \"autocompleted\" by the model.\n",
    "Use the greedy-search method and process all prefixes as a single batch.\n",
    "\n",
    "TODO: finish the implementation of the generate method by filling in the missing lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "RTkJ3hZqZsAK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "\n",
    "def generate(model, tokenizer, prefix: List[str], max_predicted: int) -> List[str]:\n",
    "  \"\"\"\n",
    "  :param model: PreTrainedModel.\n",
    "  :param tokenizer: PreTrainedTokenizer (https://huggingface.co/docs/transformers/main/main_classes/tokenizer#transformers.PreTrainedTokenizer)\n",
    "  :prefix: Batch of prefixes to be autocompleted by the language model\n",
    "  \"\"\"\n",
    "  if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "  pad_token_id = tokenizer.pad_token_id\n",
    "  bos_token_id = tokenizer.bos_token_id\n",
    "  eos_token_id = tokenizer.eos_token_id\n",
    "  device = model.device\n",
    "\n",
    "  # 1. Tokenize the prefixes and prepare them for input into the language model\n",
    "  #    Notes\n",
    "  #    - add the start token (BOS) but not the end token (EOS)\n",
    "  #    - left! pad them to the maximum length in the batch, i.e. paddding is left to the \"real\" tokens\n",
    "  #    - inputs_ids and attention_mask should be computes, and should already be stacked in the batch dim\n",
    "  #    -> this can all be done by calling the tokenizer! See: https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__\n",
    "  tokenizer.padding_side = 'left'\n",
    "  # TODO: call the tokenizer to prepare the inputs (a dictionary)\n",
    "\n",
    "  #<Dani>\n",
    "  inputs = tokenizer(prefix, \n",
    "                    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "                    padding='longest',  # Pad to the longest sequence in the batch\n",
    "                    truncation=True,  # Truncate to max_length\n",
    "                    max_length=512,  # Truncate all sequences to this length\n",
    "                    return_tensors='pt', # Return PyTorch tensors\n",
    "                    return_attention_mask=True) # Return attention mask\n",
    "  \n",
    "  # Add BOS token to the start of each sequence\n",
    "  inputs = inputs.to(device)\n",
    "  bos_token_ids = torch.full((inputs['input_ids'].size(0), 1), tokenizer.bos_token_id, dtype=torch.long, device=device)\n",
    "  inputs['input_ids'] = torch.cat([bos_token_ids, inputs['input_ids']], dim=1)\n",
    "\n",
    "  # Update the attention mask for the BOS token\n",
    "  bos_attention = torch.ones((inputs['attention_mask'].size(0), 1), dtype=torch.long, device=device)\n",
    "  inputs['attention_mask'] = torch.cat([bos_attention, inputs['attention_mask']], dim=1)\n",
    "\n",
    "  # Update position IDs to align with the new length of input_ids\n",
    "  position_ids = torch.arange(0, inputs['input_ids'].size(1), dtype=torch.long, device=device).unsqueeze(0).repeat(inputs['input_ids'].size(0), 1)\n",
    "\n",
    "\n",
    "\n",
    "  #inputs['input_ids'] = [[tokenizer.bos_token_id] + seq for seq in inputs['input_ids'].tolist()]\n",
    "\n",
    "  # Convert lists back to tensors\n",
    "  #inputs['input_ids'] = torch.tensor(inputs['input_ids'])\n",
    "\n",
    "  #</Dani>\n",
    "  \n",
    "  # inputs = tokenizer(...  # add arguments here\n",
    "  #                    return_tensors='pt')\n",
    "\n",
    "  inputs = inputs.to(device=device)\n",
    "  input_ids = inputs['input_ids']\n",
    "  attention_mask = inputs['attention_mask']\n",
    "\n",
    "  # TODO: Compute position ids based on the attention mask\n",
    "  # Note: the position ids are the indices of the positions, starting with zero and increasing for each non-padding token\n",
    "  # The positions ids for padding tokens can take any value\n",
    "\n",
    "  #<Dani>\n",
    "  # Create a mask for non-padding tokens\n",
    "  non_padding_mask = attention_mask.bool()\n",
    "\n",
    "  # Compute position ids\n",
    "  # position_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "  # Ensure position_ids has the same sequence length as non_padding_mask\n",
    "  # if position_ids.size(1) < non_padding_mask.size(1):\n",
    "  #   position_ids = torch.cat([position_ids, torch.zeros((position_ids.size(0), non_padding_mask.size(1) - position_ids.size(1)))], dim=1)\n",
    "  # elif position_ids.size(1) > non_padding_mask.size(1):\n",
    "  #   position_ids = position_ids[:, :non_padding_mask.size(1)]\n",
    "\n",
    "  # position_ids[non_padding_mask] = torch.cumsum(non_padding_mask, dim=1)[non_padding_mask] - 1\n",
    "\n",
    "  #</Dani>\n",
    "\n",
    "  # position_ids: torch.LongTensor = None\n",
    "\n",
    "\n",
    "  # Initialize some variables (nothing TODO here)\n",
    "  N = position_ids.shape[0]\n",
    "  past_key_values = None\n",
    "  unfinished_sequences = torch.ones(N, dtype=torch.bool, device=device)\n",
    "  predicted_input_ids = input_ids.clone()\n",
    "\n",
    "  while True:\n",
    "    # 2. Predict the next token logits by passing the previous inputs into the language model\n",
    "    #    Note: already computed steps can be given by past_key_values, other steps are given as input_ids\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        return_dict=True\n",
    "    )\n",
    "    # (N x |V|)\n",
    "    next_token_logits: torch.LongTensor = outputs.logits[:, -1, :]\n",
    "\n",
    "    # 3. Select the next predicted tokens by greedy search\n",
    "    # (N)\n",
    "\n",
    "    #next_tokens: torch.LongTensor = None  # TODO\n",
    "    next_tokens: torch.LongTensor = torch.argmax(next_token_logits, dim=1)\n",
    "\n",
    "    # 4. For all already finished sentences, replace the next_tokens by <PAD>\n",
    "    # next_tokens = None  # TODO\n",
    "    next_tokens = torch.where(unfinished_sequences, next_tokens, torch.tensor(pad_token_id, device=device))\n",
    "\n",
    "    # 5. Check which sentences are already finished by checking whether they contain <END>\n",
    "    # Which sentences have been finished with the predicted token (next_tokens)\n",
    "    # (N)\n",
    "    #newly_finished = None  # TODO\n",
    "    newly_finished = next_tokens == eos_token_id\n",
    "    # Which sentences, therefore, remain unfinished\n",
    "    # (N)\n",
    "    #unfinished_sequences = None  # TODO\n",
    "\n",
    "    unfinished_sequences = ~newly_finished\n",
    "\n",
    "    # 6. Concatenate the next input to predicted_input_ids, attention_mask\n",
    "    # (N x M_new) where M_new is one longer than the previous M of predicted_input_ids\n",
    "    #predicted_input_ids = None  # TODO\n",
    "\n",
    "    predicted_input_ids = torch.cat([predicted_input_ids, next_tokens.unsqueeze(dim=1)], dim=1)\n",
    "\n",
    "    # (N x M_new)\n",
    "    # attention_mask = None  # TODO: extend attention mask by ones for next step\n",
    "    attention_mask = torch.cat([attention_mask, torch.ones((N, 1), device=device)], dim=1)\n",
    "    \n",
    "    # Nothing TODO here\n",
    "    position_ids = position_ids.amax(dim=1, keepdim=True) + 1\n",
    "    input_ids = next_tokens.unsqueeze(dim=1)\n",
    "    past_key_values = outputs.past_key_values\n",
    "\n",
    "    # 7. Check if finished (nothing TODO here)\n",
    "    if predicted_input_ids.shape[1] >= max_predicted or unfinished_sequences.max() == 0:\n",
    "      break\n",
    "\n",
    "  # 8. Convert back to text (nothing TODO here)\n",
    "  generated_sentences: List[str] = tokenizer.batch_decode(predicted_input_ids, skip_special_tokens=True)\n",
    "  return generated_sentences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQzhPO9Bx6bx"
   },
   "source": [
    "## Step 2: Load a decoder language model (nothing TODO here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WSObYXZCZ77g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "decoder_model_name = \"gpt2-medium\"\n",
    "\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(decoder_model_name)\n",
    "decoder_model = AutoModelForCausalLM.from_pretrained(decoder_model_name)\n",
    "#decoder_model = decoder_model.to(device=\"cuda:0\")\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    decoder_model = decoder_model.to(device=mps_device)\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "elif torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    decoder_model = decoder_model.to(device=\"cuda:0\")\n",
    "else:\n",
    "    decoder_model = decoder_model.to(device=\"cpu\")\n",
    "    print (\"MPS / CUDA device not found. Using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3zq42ZlyQpU"
   },
   "source": [
    "## Step 3: Apply the generate method to some example text (nothing TODO here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "sN8GuYvhyYdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed sentence 0: \"This sentences is about\"\n",
      "-------------------------------------------------------\n",
      "\n",
      "This sentences is about the relationship between the two.\n",
      "\n",
      "The first sentence is about the relationship between the two.\n",
      "\n",
      "The second sentence is about the relationship between the two.\n",
      "\n",
      "The third sentence is about the relationship between the\n",
      "\n",
      "=======================================================\n",
      "\n",
      "\n",
      "Completed sentence 1: \"Can you complete this sentence?\"\n",
      "-------------------------------------------------------\n",
      "\n",
      "Can you complete this sentence?\n",
      "\n",
      "\"I'm not sure if I can do this.\"\n",
      "\n",
      "I'm not sure if I can do this.\n",
      "\n",
      "I'm not sure if I can do this.\n",
      "\n",
      "I'm not sure\n",
      "\n",
      "=======================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can play around with different texts here\n",
    "prefix_1 = \"This sentences is about\"\n",
    "prefix_2 = \"Can you complete this sentence?\"\n",
    "prefixes = [prefix_1, prefix_2]\n",
    "\n",
    "generated_sentences = generate(decoder_model, decoder_tokenizer, prefixes, max_predicted=50)\n",
    "\n",
    "for i, (prefix, sent) in enumerate(zip(prefixes, generated_sentences)):\n",
    "  print(f'Completed sentence {i}: \"{prefix}\"')\n",
    "  print(\"-------------------------------------------------------\\n\")\n",
    "  print(sent)\n",
    "  print(\"\\n=======================================================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_M43bZ4s9UCH"
   },
   "source": [
    "As you can see, the results tend to be quite repetetive.\n",
    "This is why greedy search is typically not used in practice.\n",
    "Common alternatives include beam search and sampling from the distribution of next tokens. While this is not part of the mandatory exercise, you can try to implement other generation methods as part of a bonus task and see how it can improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWu4pDaJy2XG"
   },
   "source": [
    "## Extra Task (not graded): Implement a beam search generate function and apply it\n",
    "Re-implement the generate method but instead of doing greedy search, use beam search instead.\n",
    "\n",
    "Note that this is a completely optional and non-graded task and will not be discussed in the soutions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zewLo1VLzfpg"
   },
   "outputs": [],
   "source": [
    "def beam_search_generate(model, tokenizer, prefix: List[str], max_predicted: int, num_beams: int = 5) -> List[str]:\n",
    "  pass  # TODO"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "dz_Is5Bw_PZC",
    "I62jANWNvkzp",
    "fBPyy6E5wpVC"
   ],
   "provenance": [
    {
     "file_id": "1g8x2r2zKvDgL_YO7WsixKwCq80xoFod3",
     "timestamp": 1715611942175
    },
    {
     "file_id": "1YVhdEh9KDSoaPWyq8WU0b4tKr5XfTkKp",
     "timestamp": 1683023138687
    },
    {
     "file_id": "1I8W6SJAHopCX3Ziq5DNZJxpYTImENO7r",
     "timestamp": 1682088432737
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
