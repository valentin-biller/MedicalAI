{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ9sen2-10wB"
   },
   "source": [
    "# Hand-in Group\n",
    "TODO: State the names of all group members and TUM-IDs\n",
    "\n",
    "* Bhat Maitri: 03756699\n",
    "* Biller Valentin: 03724152\n",
    "* Szabo Daniel: 03726951"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dz_Is5Bw_PZC"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ALExTEN17Jv"
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3ZN2hZNGvkBd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from fsspec[http]>=2021.11.1->datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: six in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tokenizers in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: transformers in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: stanza in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (1.8.2)\n",
      "Requirement already satisfied: emoji in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from stanza) (2.11.1)\n",
      "Requirement already satisfied: numpy in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from stanza) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from stanza) (3.20.3)\n",
      "Requirement already satisfied: requests in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from stanza) (2.31.0)\n",
      "Requirement already satisfied: networkx in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from stanza) (3.1)\n",
      "Requirement already satisfied: toml in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from stanza) (0.10.2)\n",
      "Requirement already satisfied: torch>=1.3.0 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from stanza) (2.3.0.dev20240111)\n",
      "Requirement already satisfied: tqdm in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from stanza) (4.65.0)\n",
      "Requirement already satisfied: filelock in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from torch>=1.3.0->stanza) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from torch>=1.3.0->stanza) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from torch>=1.3.0->stanza) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from torch>=1.3.0->stanza) (2023.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from requests->stanza) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from requests->stanza) (2024.2.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/valentinbiller/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66028aba8b91422885693bcc5c981f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 19:32:18 INFO: Downloaded file to /Users/valentinbiller/stanza_resources/resources.json\n",
      "2024-05-13 19:32:18 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-05-13 19:32:19 INFO: File exists: /Users/valentinbiller/stanza_resources/en/default.zip\n",
      "2024-05-13 19:32:23 INFO: Finished downloading models and saved to /Users/valentinbiller/stanza_resources\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install tokenizers\n",
    "!pip install transformers\n",
    "!pip install stanza\n",
    "# -- Initialize Stanza --\n",
    "import stanza\n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGa7gWpX2ArF"
   },
   "source": [
    "## Download Datasets\n",
    "Note: In case the following script fails because the medical transcriptions dataset is no longer available it can be downloaded from https://www.kaggle.com/tboyle10/medicaltranscriptions ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UGhgMCGa2C7F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 9051 files from the OpenI dataset into \"open_i/ecgen-radiology\"\n",
      "/Users/valentinbiller/anaconda3/lib/python3.9/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1E0hm3r9bwK8cujyIcOjp_y-ZEPt1HBjn\n",
      "To: /Users/valentinbiller/Library/Mobile Documents/com~apple~CloudDocs/• Uni/M.Sc. Robotics, Cognition, Intelligence/Artificial Intelligence in Medicine II/Homework/Code/NLP/medical_transcriptions/medical_transcriptions.zip\n",
      "100%|██████████████████████████████████████| 5.08M/5.08M [00:00<00:00, 6.92MB/s]\n",
      "Archive:  medical_transcriptions/medical_transcriptions.zip\n",
      "  inflating: medical_transcriptions/mtsamples.csv  \n",
      "Downloaded medical transriptions dataset\n"
     ]
    }
   ],
   "source": [
    "# -- OPEN-I dataset --\n",
    "!mkdir -p open_i\n",
    "!wget -q -N -P open_i https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz\n",
    "!tar zxf open_i/NLMCXR_reports.tgz -C open_i\n",
    "import glob\n",
    "num_files = len(glob.glob(\"open_i/ecgen-radiology/*.xml\"))\n",
    "print(f'Downloaded {num_files} files from the OpenI dataset into \"open_i/ecgen-radiology\"')\n",
    "\n",
    "# -- Transcriptions Dataset --\n",
    "!mkdir -p medical_transcriptions\n",
    "!gdown --id 1E0hm3r9bwK8cujyIcOjp_y-ZEPt1HBjn -O medical_transcriptions/medical_transcriptions.zip\n",
    "!unzip -o medical_transcriptions/medical_transcriptions.zip -d medical_transcriptions\n",
    "print(f'Downloaded medical transriptions dataset')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbcFrCre_K84"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I62jANWNvkzp"
   },
   "source": [
    "# Task 1: Pre-Processing and Vocabulary Construction (not graded!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7Oy5fnA1iwl"
   },
   "source": [
    "## Note\n",
    "- Task 1 is not graded, i.e. it is not mandatory to solve this task. It insteads serves soleley to show how pre-processing and tokenization can be done.\n",
    "- Note that tasks 2 and 3 are mandatory!\n",
    "\n",
    "## Goals\n",
    "- Understand tools for text pre-processing\n",
    "- Apply text normalization and sentence splitting to a medical corpus\n",
    "- Train a tokenizer on a medical corpus\n",
    "\n",
    "## Tools\n",
    "- [Huggingface Datasets](https://huggingface.co/docs/datasets/): Library for efficient loading, saving and processing of text datasets\n",
    "  - Note: In many cases using pandas is sufficient for text datasets (as they often fit into memory), but Hugginface Datasets provides some usful features like batched mapping of samples.\n",
    "- [Huggingface Tokenizers](https://huggingface.co/docs/tokenizers/python/latest/): Library for text normalization and tokenization\n",
    "- [Stanza](https://stanfordnlp.github.io/stanza/): Text processing toolkit from the Stanford NLP group\n",
    "   - usefull for sentence splitting\n",
    "\n",
    "## Dataset\n",
    "OpenI [[Website](https://openi.nlm.nih.gov/faq#collection)] [[Download](https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz)]\n",
    "- Contains reports and scans\n",
    "- We only use the reports from this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zYPRTlpwVsj"
   },
   "source": [
    "## Step 1 - Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyjpv7e62Sg_"
   },
   "source": [
    "In this step the goal is to normalize text from the corpus.\n",
    "\n",
    "TODO: Implement a function for normalizing a single text sample (e.g. a report from). This function will later be applied to all sample of the corpus.\n",
    "You can decide which text normalization you find appropriate and implement one or some of them. Typical examples are:\n",
    "- Unicode normalization\n",
    "- Stripping accents\n",
    "- Lowercasing\n",
    "- Removing control characters\n",
    "- Normalizing whitespace characters (i.e. replacing all whitespaces like tabs, ... by the default whitespace) and removing redundant whitespaces\n",
    "- Removing or normalizing special characters\n",
    "\n",
    "Note: you may implement each normalizazion using regex/string operations or you can use normalizers from https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywVCYPWUwpl8"
   },
   "outputs": [],
   "source": [
    "from tokenizers.normalizers import *\n",
    "\n",
    "# TODO: implement normalize_text using regex/string operations and/or normalizers from tokenizers.normalizers\n",
    "def normalize_text(text: str) -> str:\n",
    "  return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHBm4H4NDZxe"
   },
   "outputs": [],
   "source": [
    "# apply normalization to some example text\n",
    "# you can play around with different example texts to understand the effects of each normalizer\n",
    "example_text = 'Schöner Tag'\n",
    "\n",
    "print(normalize_text(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8XW3OmixY2y"
   },
   "source": [
    "## Step 2 - Sentence Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hot5HA7f2Po4"
   },
   "source": [
    "In this step the goal is to split each section into sentences.\n",
    "\n",
    "TODO: Implement a function for splitting a single report section (given as a single string) into sentences (returned as a list of strings, one string for each sentence). This function will later be applied to all samples of the corpus.\n",
    "For sentence splitting the stanza tokenizer should be used as it provides a more robust solution compared to splitting on pre-defined characters (like \".\") .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wihSdqW52QKa"
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "from typing import List\n",
    "\n",
    "stanza_tokenizer = stanza.Pipeline('en', processors='tokenize', use_gpu=True)\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "  # TODO: use the stanza_tokenizer to split the text string into sentences\n",
    "  # See https://stanfordnlp.github.io/stanza/tokenize.html#tokenization-and-sentence-segmentation\n",
    "  return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2sG1E5KupYh"
   },
   "outputs": [],
   "source": [
    "# apply sentence splitting on some example text\n",
    "example_text = \"This is the first sentence. Let's try something else, e.g. this. Another sentence?\"\n",
    "\n",
    "print(split_sentences(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WHrFJhNxiJB"
   },
   "source": [
    "## Step 3 - Apply text normalization and sentence splitting to the Open-I dataset (nothing TODO here)\n",
    "We now apply text normalization and sentences splitting to the Open-I dataset.\n",
    "Each sample of the Open-I dataset contains a report with (besides others) a \"findings\" and \"impression\" section.\n",
    "\n",
    "We make use of the huggingface datasets library for efficiently applying text normalization and sentence splitting to both sections of each sample of the Open-I dataset.\n",
    "This step is already implemented but utilizes the functions implemented in Step 1 and 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCmeMqyu20h5"
   },
   "source": [
    "1. Load the Open-I dataset as a huggingface Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUiAQ9A52-iH"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_open_i_dataset():\n",
    "  # Load from xml-files\n",
    "  uIds = []\n",
    "  findings_sections = []\n",
    "  impression_sections = []\n",
    "  for file in tqdm(glob.glob(\"open_i/ecgen-radiology/*.xml\")):\n",
    "    tree = ET.parse(file)\n",
    "\n",
    "    uId = tree.find(\"./uId\").get('id')\n",
    "    uIds.append(uId)\n",
    "\n",
    "    finding = tree.find(\".//AbstractText[@Label='FINDINGS']\").text\n",
    "    findings_sections.append(finding if finding is not None else '')\n",
    "\n",
    "    imp = tree.find(\".//AbstractText[@Label='IMPRESSION']\").text\n",
    "    impression_sections.append(imp if imp is not None else '')\n",
    "\n",
    "  assert len(uIds) > 0, 'No data found. Download the data first.'\n",
    "  return Dataset.from_dict({\n",
    "      'uId': uIds,\n",
    "      'findings': findings_sections,\n",
    "      'impression': impression_sections\n",
    "  })\n",
    "\n",
    "data = load_open_i_dataset()\n",
    "print(data) # Overview over the dataset\n",
    "print(data[0])  # The first sample of the dataset\n",
    "# Save to be loaded later\n",
    "data.save_to_disk('open_i_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9V0DosV3VuP"
   },
   "source": [
    "2. Apply normalize_text to the findings and impression sections using the map function of datasets.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbAjQch1_Z1J"
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "data: Dataset = load_from_disk('open_i_raw')\n",
    "def normalize_text_samples(sample: dict) -> dict:\n",
    "  return {\n",
    "      'findings_normalized': normalize_text(sample['findings']),\n",
    "      'impression_normalized': normalize_text(sample['impression'])\n",
    "  }\n",
    "data = data.map(normalize_text_samples)\n",
    "\n",
    "print(data) # Overview over the dataset\n",
    "print(data[0])  # The first sample of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSXv8QNeAn-b"
   },
   "source": [
    "3. Apply split_sentences to the findings and impression sections using the map function of datasets.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EY4-GRj3YiZb"
   },
   "outputs": [],
   "source": [
    "def split_sample_sentences(sample: dict) -> dict:\n",
    "  return {\n",
    "      'findings_sentences': split_sentences(sample['findings']),\n",
    "      'impression_sentences': split_sentences(sample['impression'])\n",
    "  }\n",
    "\n",
    "split_dataset = data.map(split_sample_sentences)\n",
    "\n",
    "print(split_dataset)\n",
    "print(split_dataset[0])\n",
    "split_dataset.save_to_disk('open_i_split')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GA3p83wOwdy8"
   },
   "source": [
    "## Step 4 - Vocabulary Creation: Train a tokenizer\n",
    "In this step the goal is to learn a vocabulary (i.e. tokenizer) from the Open-I sentences. We therefore extract a list of all sentences (from findings and impression section) in the dataset. This list is then used for learning the vocabulary.\n",
    "\n",
    "TODO: Extract all sentences and store them in all_sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNp27c98Fs4V"
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "split_dataset: Dataset = load_from_disk('open_i_split')\n",
    "print(split_dataset)\n",
    "\n",
    "# TODO: extract the list of all sentences in the whole dataset (from both sections)\n",
    "all_sentences: List[str] = None\n",
    "\n",
    "print(f'\\nTotal number of sentences: {len(all_sentences)}')\n",
    "print(f'Total number of words (by whitespaces): {sum(len(sent.split()) for sent in all_sentences)}')\n",
    "num_unique_words = len(set(word for sent in all_sentences for word in sent.split()))\n",
    "print(f'Total number of different words: {num_unique_words}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOa8C0_4WdOi"
   },
   "source": [
    "We no define the initial alphabet (optional, by default the alphabet is derived from the characters present in the dataset) and the size of the vocabulary we want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8uzL_qe38is"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# All ascii characters (lowercase only) + digits\n",
    "initial_alphabet = list(string.ascii_lowercase) + list(string.digits)\n",
    "print(f'Initial alphabet size: {len(initial_alphabet)}')\n",
    "\n",
    "vocab_size = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfi-3JroWpCb"
   },
   "source": [
    "Now we use the huggingface tokenizer library to learn a WordPiece tokenizer. This is already implemented but you can try with different arguments or also try other tokenizers, e.g. BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmWoQGHy3da2"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers, processors\n",
    "\n",
    "special_tokens = [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"]\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token='<UNK>'))\n",
    "# we use a lowercase vocabulary\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"<BOS> $0 <EOS>\",\n",
    "    pair=\"<BOS> $A <EOS> <BOS> $B <EOS>\",\n",
    "    special_tokens=[(\"<BOS>\", 2), (\"<EOS>\", 3)],\n",
    ")\n",
    "tokenizer.decoder = decoders.WordPiece()\n",
    "\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    initial_alphabet=initial_alphabet,\n",
    "    special_tokens=special_tokens,\n",
    ")\n",
    "\n",
    "# now train it\n",
    "tokenizer.train_from_iterator(all_sentences, trainer=trainer)\n",
    "\n",
    "print(f'Vocab size: {tokenizer.get_vocab_size()}')\n",
    "print('Vocab: \\n', tokenizer.get_vocab())\n",
    "\n",
    "tokenizer.save('tokenizer.json', pretty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5O4vgqeFtTN"
   },
   "source": [
    "### Inspection of the created vocabulary\n",
    "You can now take a look at the created vocabulary by opening the file \"tokenizer.json\".\n",
    "\n",
    "You can also try around tokenizing some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YYXc8_t8mvF"
   },
   "outputs": [],
   "source": [
    "example_section = split_dataset[0]['findings_sentences']\n",
    "example_sentence = example_section[0]\n",
    "\n",
    "print(example_sentence)\n",
    "\n",
    "encoded = tokenizer.encode(example_sentence)\n",
    "print(f'Encoded tokens: {encoded.tokens}')\n",
    "print(f'Encoded token ids: {encoded.ids}')\n",
    "print(f'Decoded again: {tokenizer.decode(encoded.ids)}')\n",
    "print(f'Decoded (keep special tokens): {tokenizer.decode(encoded.ids, skip_special_tokens=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBPyy6E5wpVC"
   },
   "source": [
    "# Task 2: Training a Text Classifier (graded!)\n",
    "## Goals\n",
    "- Understand how pre-trained language models can be utilized for downstream tasks\n",
    "\n",
    "## Tools\n",
    "- [Huggingface Transformers](https://huggingface.co/transformers/): Library for pre-trained transformer-based language models\n",
    "\n",
    "## Dataset\n",
    "Medical Transcriptions [[Kaggle](https://www.kaggle.com/tboyle10/medicaltranscriptions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9olvk3WNS5Uc"
   },
   "source": [
    "## Step 1 - Load the Medical Transactions Dataset (Nothing TODO here)\n",
    "We first load the Medical Transcriptions dataset as a huggingface dataset. The dataset is split into train and test set, so the returned type is a datasets.DatasetDict, which acts like a dictionary of datasets.Dataset but provides utility functions, e.g. for mapping all datasets of the dict using the map method.\n",
    "This step is already implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e05T0u7US1jt"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_transciptions_dataset() -> Tuple[DatasetDict, List[str]]:\n",
    "    df = pd.read_csv('medical_transcriptions/mtsamples.csv')\n",
    "    df = df.drop(['Unnamed: 0'],axis=1,)\n",
    "\n",
    "    counts = df['medical_specialty'].value_counts()\n",
    "    print(f'Original counts:\\n{counts}')\n",
    "    dropped_specialities = [k for k, v in counts.items() if v < 100]\n",
    "    for dropped_speciality in dropped_specialities:\n",
    "      df = df[df['medical_specialty'] != dropped_speciality]\n",
    "    df.dropna(inplace=True)\n",
    "    counts = df['medical_specialty'].value_counts()\n",
    "    print(f'Counts after removing small specialities:\\n{counts}')\n",
    "\n",
    "    df['medical_specialty'] = df['medical_specialty'].astype('category')\n",
    "    class_names = df['medical_specialty'].cat.categories.tolist()\n",
    "    print(f'Class names : {class_names}')\n",
    "    df['medical_specialty'] = df['medical_specialty'].cat.codes\n",
    "\n",
    "    train, test = train_test_split(df, stratify=df['medical_specialty'], test_size=0.25)\n",
    "    dataset = DatasetDict({'train': Dataset.from_pandas(train), 'test': Dataset.from_pandas(test)})\n",
    "    dataset = dataset.remove_columns(['__index_level_0__'])\n",
    "\n",
    "    return dataset, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iKtCzBSp1ADK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original counts:\n",
      " Surgery                          1103\n",
      " Consult - History and Phy.        516\n",
      " Cardiovascular / Pulmonary        372\n",
      " Orthopedic                        355\n",
      " Radiology                         273\n",
      " General Medicine                  259\n",
      " Gastroenterology                  230\n",
      " Neurology                         223\n",
      " SOAP / Chart / Progress Notes     166\n",
      " Obstetrics / Gynecology           160\n",
      " Urology                           158\n",
      " Discharge Summary                 108\n",
      " ENT - Otolaryngology               98\n",
      " Neurosurgery                       94\n",
      " Hematology - Oncology              90\n",
      " Ophthalmology                      83\n",
      " Nephrology                         81\n",
      " Emergency Room Reports             75\n",
      " Pediatrics - Neonatal              70\n",
      " Pain Management                    62\n",
      " Psychiatry / Psychology            53\n",
      " Office Notes                       51\n",
      " Podiatry                           47\n",
      " Dermatology                        29\n",
      " Cosmetic / Plastic Surgery         27\n",
      " Dentistry                          27\n",
      " Letters                            23\n",
      " Physical Medicine - Rehab          21\n",
      " Sleep Medicine                     20\n",
      " Endocrinology                      19\n",
      " Bariatrics                         18\n",
      " IME-QME-Work Comp etc.             16\n",
      " Chiropractic                       14\n",
      " Rheumatology                       10\n",
      " Diets and Nutritions               10\n",
      " Speech - Language                   9\n",
      " Autopsy                             8\n",
      " Lab Medicine - Pathology            8\n",
      " Allergy / Immunology                7\n",
      " Hospice - Palliative Care           6\n",
      "Name: medical_specialty, dtype: int64\n",
      "Counts after removing small specialities:\n",
      " Surgery                          1021\n",
      " Orthopedic                        303\n",
      " Cardiovascular / Pulmonary        280\n",
      " Radiology                         251\n",
      " Consult - History and Phy.        234\n",
      " Gastroenterology                  195\n",
      " Neurology                         168\n",
      " General Medicine                  146\n",
      " SOAP / Chart / Progress Notes     142\n",
      " Urology                           140\n",
      " Obstetrics / Gynecology           130\n",
      " Discharge Summary                  77\n",
      "Name: medical_specialty, dtype: int64\n",
      "Class names : [' Cardiovascular / Pulmonary', ' Consult - History and Phy.', ' Discharge Summary', ' Gastroenterology', ' General Medicine', ' Neurology', ' Obstetrics / Gynecology', ' Orthopedic', ' Radiology', ' SOAP / Chart / Progress Notes', ' Surgery', ' Urology']\n"
     ]
    }
   ],
   "source": [
    "dataset, class_names = load_transciptions_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NMwK0iiUvhUV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  2315\n",
      "Test samples:  772\n",
      "Columns:  {'description': Value(dtype='string', id=None), 'medical_specialty': Value(dtype='int8', id=None), 'sample_name': Value(dtype='string', id=None), 'transcription': Value(dtype='string', id=None), 'keywords': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print('Training samples: ', len(dataset['train']['transcription']))\n",
    "print('Test samples: ', len(dataset['test']['transcription']))\n",
    "print('Columns: ', dataset['train'].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKthj9SypvMB"
   },
   "source": [
    "## Step 2 - Compute sentence embeddings using pre-trained language model\n",
    "Our goal is to use a pre-trained language model (from the huggingface transformers library) to encode sentences into sentence representations. Therefore, each sentence is tokenized and passed to the language model. The resulting contextualized token representations (i.e. the outputs of the last hidden layer of the language model) are then globally pooled to get sentence-level representations. As we use a pre-trained model, no training is involved in this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTggsRsJHxCM"
   },
   "source": [
    "### Specify pre-trained language model\n",
    "TODO: Decide which pre-trained language model you want to use and specify its name here.\n",
    "You can search for models at the huggingface model hub: https://huggingface.co/models . You can either use standard models, e.g. BERT, or use biomedcial models.\n",
    "\n",
    "When you decided for a model, copy the name of the model from the URL (removing only https://huggingface.co/) and insert it as model_name here.\n",
    "For more reference on how to use the tokenizer have a look at https://huggingface.co/docs/transformers/preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "y1h6SqqcHwIZ"
   },
   "outputs": [],
   "source": [
    "# TODO: Insert model name here\n",
    "model_name = 'google-bert/bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "k85a1cxn1RSG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer (nothing TODO)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YSaLG4Y6H113"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valentinbiller/anaconda3/lib/python3.9/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the model (nothing TODO) and inspect it\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-U1wMp2SHInc"
   },
   "source": [
    "### Experiment with tokenizer and language model (nothing to implement here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Aipe5EhsTaIf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example text:  ['PREOPERATIVE DIAGNOSES:,1.  Status post multiple trauma/motor vehicle accident.,2.  Acute respiratory failure.,3.  Acute respiratory distress/ventilator asynchrony.,4.  Hypoxemia.,5.  Complete atelectasis of left lung.,POSTOPERATIVE DIAGNOSES:,1.  Status post multiple trauma/motor vehicle accident.,2.  Acute respiratory failure.,3.  Acute respiratory distress/ventilator asynchrony.,4.  Hypoxemia.,5.  Complete atelectasis of left lung.,6.  Clots partially obstructing the endotracheal tube and completely obstructing the entire left main stem and entire left bronchial system.,PROCEDURE PERFORMED: ,Emergent fiberoptic plus bronchoscopy with lavage.,LOCATION OF PROCEDURE:  ,ICU.  Room #164.,ANESTHESIA/SEDATION:,  Propofol drip, Brevital 75 mg, morphine 5 mg, and Versed 8 mg.,HISTORY,:  The patient is a 44-year-old male who was admitted to ABCD Hospital on 09/04/03 status post MVA with multiple trauma and subsequently diagnosed with multiple spine fractures as well as bilateral pulmonary contusions, requiring ventilatory assistance.  The patient was noted with acute respiratory distress on ventilator support with both ventilator asynchrony and progressive desaturation.  Chest x-ray as noted above revealed complete atelectasis of the left lung.  The patient was subsequently sedated and received one dose of paralytic as noted above followed by emergent fiberoptic flexible bronchoscopy.,PROCEDURE DETAIL,:  A bronchoscope was inserted through the oroendotracheal tube, which was partially obstructed with blood clots.  These were lavaged with several aliquots of normal saline until cleared.  The bronchoscope required removal because the tissue/clots were obstructing the bronchoscope.  The bronchoscope was reinserted on several occasions until cleared and advanced to the main carina.  The endotracheal tube was noted to be in good position.  The bronchoscope was advanced through the distal trachea.  There was a white tissue completely obstructing the left main stem at the carina.  The bronchoscope was advanced to this region and several aliquots of normal saline lavage were instilled and suctioned.  Again this partially obstructed the bronchoscope requiring several times removing the bronchoscope to clear the lumen.  The bronchoscope subsequently was advanced into the left mainstem and subsequently left upper and lower lobes.  There was diffuse mucus impactions/tissue as well as intermittent clots.  There was no evidence of any active bleeding noted.  Bronchoscope was adjusted and the left lung lavaged until no evidence of any endobronchial obstruction is noted.  Bronchoscope was then withdrawn to the main carina and advanced into the right bronchial system.  There is no plugging or obstruction of the right bronchial system.  The bronchoscope was then withdrawn to the main carina and slowly withdrawn as the position of endotracheal tube was verified, approximately 4 cm above the main carina.  The bronchoscope was then completely withdrawn as the patient was maintained on ventilator support during and postprocedure.  Throughout the procedure, pulse oximetry was greater than 95% throughout.  There is no hemodynamic instability or variability noted during the procedure.  Postprocedure chest x-ray is pending at this time.', \"PROCEDURE:,  Gastroscopy.,PREOPERATIVE DIAGNOSIS:,  Dysphagia and globus.,POSTOPERATIVE DIAGNOSIS: , Normal.,MEDICATIONS:,  MAC.,DESCRIPTION OF PROCEDURE: , The Olympus gastroscope was introduced through the oropharynx and passed carefully through the esophagus and stomach, and then through the gastrojejunal anastomosis into the efferent jejunal loop.  The preparation was good and all surfaces were well seen.  The hypopharynx was normal with no evidence of inflammation.  The esophagus had a normal contour and normal mucosa throughout with no sign of stricturing or inflammation or exudate.  The GE junction was located at 39 cm from the incisors and appeared normal with no evidence of reflux, damage, or Barrett's.  Below this there was a small gastric pouch measuring 6 cm with intact mucosa and no retained food.  The gastrojejunal anastomosis was patent measuring about 12 mm, with no inflammation or ulceration.  Beyond this there was a side-to-side gastrojejunal anastomosis with a short afferent blind end and a normal efferent end with no sign of obstruction or inflammation.  The scope was withdrawn and the patient was sent to recovery room.  She tolerated the procedure well.,FINAL DIAGNOSES:,1.  Normal post-gastric bypass anatomy.,2.  No evidence of inflammation or narrowing to explain her symptoms.\"]\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "input ids:  [[101, 3653, 25918, 8082, 22939, 26745, 8583, 1024, 1010, 1015, 1012, 3570, 2695, 3674, 12603, 1013, 5013, 4316, 4926, 1012, 1010, 1016, 1012, 11325, 16464, 4945, 1012, 1010, 1017, 1012, 11325, 16464, 12893, 1013, 18834, 11733, 4263, 2004, 6038, 2818, 4948, 2100, 1012, 1010, 1018, 1012, 1044, 22571, 11636, 17577, 1012, 1010, 1019, 1012, 3143, 8823, 2571, 25572, 6190, 1997, 2187, 11192, 1012, 1010, 2695, 25918, 8082, 22939, 26745, 8583, 1024, 1010, 1015, 1012, 3570, 2695, 3674, 12603, 1013, 5013, 4316, 4926, 1012, 1010, 1016, 1012, 11325, 16464, 4945, 1012, 1010, 1017, 1012, 11325, 16464, 12893, 1013, 18834, 11733, 4263, 2004, 6038, 2818, 4948, 2100, 1012, 1010, 1018, 1012, 1044, 22571, 11636, 17577, 1012, 1010, 1019, 1012, 3143, 8823, 2571, 25572, 6190, 1997, 2187, 11192, 1012, 1010, 1020, 1012, 18856, 12868, 6822, 27885, 3367, 6820, 11873, 1996, 2203, 4140, 22648, 20192, 2140, 7270, 1998, 3294, 27885, 3367, 6820, 11873, 1996, 2972, 2187, 2364, 7872, 1998, 2972, 2187, 22953, 12680, 4818, 2291, 1012, 1010, 7709, 2864, 1024, 1010, 12636, 3372, 11917, 7361, 4588, 4606, 22953, 12680, 2891, 3597, 7685, 2007, 13697, 3351, 1012, 1010, 3295, 1997, 7709, 1024, 1010, 24582, 2226, 1012, 2282, 1001, 17943, 1012, 1010, 2019, 25344, 1013, 7367, 20207, 1024, 1010, 17678, 11253, 4747, 27304, 1010, 7987, 6777, 18400, 4293, 11460, 1010, 22822, 20738, 1019, 11460, 1010, 1998, 7893, 2094, 1022, 11460, 1012, 1010, 2381, 1010, 1024, 1996, 5776, 2003, 1037, 4008, 1011, 2095, 1011, 2214, 3287, 2040, 2001, 4914, 2000, 5925, 2094, 2902, 2006, 5641, 1013, 5840, 1013, 6021, 3570, 2695, 19842, 2050, 2007, 3674, 12603, 1998, 3525, 11441, 2007, 3674, 8560, 28929, 2004, 2092, 2004, 17758, 21908, 9530, 5809, 8496, 1010, 9034, 18834, 11733, 7062, 5375, 1012, 1996, 5776, 2001, 3264, 2007, 11325, 16464, 12893, 2006, 18834, 11733, 4263, 2490, 2007, 2119, 18834, 11733, 4263, 2004, 6038, 2818, 4948, 2100, 1998, 6555, 4078, 4017, 18924, 1012, 3108, 1060, 1011, 4097, 2004, 3264, 2682, 3936, 3143, 8823, 2571, 25572, 6190, 1997, 1996, 2187, 11192, 1012, 1996, 5776, 2001, 3525, 7367, 13701, 2094, 1998, 2363, 2028, 13004, 1997, 11498, 2135, 4588, 2004, 3264, 2682, 2628, 2011, 12636, 3372, 11917, 7361, 4588, 12379, 22953, 12680, 2891, 3597, 7685, 1012, 1010, 7709, 6987, 1010, 1024, 1037, 22953, 12680, 2891, 16186, 2001, 12889, 2083, 1996, 20298, 10497, 4140, 22648, 20192, 2140, 7270, 1010, 2029, 2001, 6822, 27885, 3367, 6820, 10985, 2007, 2668, 18856, 12868, 1012, 2122, 2020, 13697, 5999, 2007, 2195, 4862, 28940, 12868, 1997, 3671, 28413, 2127, 5985, 1012, 1996, 22953, 12680, 2891, 16186, 3223, 8208, 2138, 1996, 8153, 1013, 18856, 12868, 2020, 27885, 3367, 6820, 11873, 1996, 22953, 12680, 2891, 16186, 1012, 1996, 22953, 12680, 2891, 16186, 2001, 19222, 28728, 2006, 2195, 6642, 2127, 5985, 1998, 3935, 2000, 1996, 2364, 2482, 3981, 1012, 1996, 2203, 4140, 22648, 20192, 2140, 7270, 2001, 3264, 2000, 2022, 1999, 2204, 2597, 1012, 1996, 22953, 12680, 2891, 16186, 2001, 3935, 2083, 1996, 29333, 19817, 15395, 2050, 1012, 2045, 2001, 1037, 2317, 8153, 3294, 27885, 3367, 6820, 11873, 1996, 2187, 2364, 7872, 2012, 1996, 2482, 3981, 1012, 1996, 22953, 12680, 2891, 16186, 2001, 3935, 2000, 2023, 102], [101, 7709, 1024, 1010, 3806, 13181, 9363, 7685, 1012, 1010, 3653, 25918, 8082, 11616, 1024, 1010, 1040, 7274, 21890, 10440, 1998, 1043, 4135, 8286, 1012, 1010, 2695, 25918, 8082, 11616, 1024, 1010, 3671, 1012, 1010, 20992, 1024, 1010, 6097, 1012, 1010, 6412, 1997, 7709, 1024, 1010, 1996, 26742, 3806, 13181, 26127, 2001, 3107, 2083, 1996, 20298, 21890, 18143, 2595, 1998, 2979, 5362, 2083, 1996, 9686, 7361, 3270, 12349, 1998, 4308, 1010, 1998, 2059, 2083, 1996, 3806, 13181, 6460, 19792, 2389, 9617, 16033, 15530, 2483, 2046, 1996, 1041, 12494, 4765, 15333, 19792, 2389, 7077, 1012, 1996, 7547, 2001, 2204, 1998, 2035, 9972, 2020, 2092, 2464, 1012, 1996, 1044, 22571, 7361, 8167, 6038, 2595, 2001, 3671, 2007, 2053, 3350, 1997, 21733, 1012, 1996, 9686, 7361, 3270, 12349, 2018, 1037, 3671, 9530, 21163, 1998, 3671, 14163, 13186, 2050, 2802, 2007, 2053, 3696, 1997, 9384, 12228, 2030, 21733, 2030, 4654, 14066, 2618, 1012, 1996, 16216, 5098, 2001, 2284, 2012, 4464, 4642, 2013, 1996, 4297, 19565, 2869, 1998, 2596, 3671, 2007, 2053, 3350, 1997, 25416, 25148, 1010, 4053, 1010, 2030, 12712, 1005, 1055, 1012, 2917, 2023, 2045, 2001, 1037, 2235, 3806, 12412, 21445, 9854, 1020, 4642, 2007, 10109, 14163, 13186, 2050, 1998, 2053, 6025, 2833, 1012, 1996, 3806, 13181, 6460, 19792, 2389, 9617, 16033, 15530, 2483, 2001, 7353, 9854, 2055, 2260, 3461, 1010, 2007, 2053, 21733, 2030, 17359, 19357, 3508, 1012, 3458, 2023, 2045, 2001, 1037, 2217, 1011, 2000, 1011, 2217, 3806, 13181, 6460, 19792, 2389, 9617, 16033, 15530, 2483, 2007, 1037, 2460, 21358, 7512, 4765, 6397, 2203, 1998, 1037, 3671, 1041, 12494, 4765, 2203, 2007, 2053, 3696, 1997, 27208, 2030, 21733, 1012, 1996, 9531, 2001, 9633, 1998, 1996, 5776, 2001, 2741, 2000, 7233, 2282, 1012, 2016, 25775, 1996, 7709, 2092, 1012, 1010, 2345, 22939, 26745, 8583, 1024, 1010, 1015, 1012, 3671, 2695, 1011, 3806, 12412, 11826, 13336, 1012, 1010, 1016, 1012, 2053, 3350, 1997, 21733, 2030, 21978, 2000, 4863, 2014, 8030, 1012, 102]]\n",
      "attention mask:  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "length:  512\n"
     ]
    }
   ],
   "source": [
    "# Tokenize some example text and inspect the results to understand the outputs of the tokenizer\n",
    "sample_text = dataset['train'][:2]['transcription']  # batch of 2 samples of text\n",
    "print('example text: ', sample_text)\n",
    "toknized = tokenizer(sample_text, truncation=True, max_length=512)\n",
    "print(toknized.keys())\n",
    "print('input ids: ', toknized['input_ids'])\n",
    "print('attention mask: ', toknized['attention_mask'])\n",
    "print('length: ', len(toknized['input_ids'][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XK7Zzjyp3yJ7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "input ids:  tensor([[  101,  3653, 25918,  ...,  2000,  2023,   102],\n",
      "        [  101,  7709,  1024,  ...,     0,     0,     0]])\n",
      "attention mask:  tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "shape:  torch.Size([2, 512])\n",
      "output shape:  torch.Size([2, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "# Feed some example data to understand the outputs of the language model\n",
    "\n",
    "# pad the batch and convert it to Pytorch\n",
    "# return_tensors='pt' => return the tokenized values as PyTorch tensors instead of lists\n",
    "x = tokenizer.pad(toknized, return_tensors='pt')\n",
    "print(x.keys())\n",
    "print('input ids: ', x['input_ids'])\n",
    "print('attention mask: ', x['attention_mask'])\n",
    "print('shape: ', x['input_ids'].shape)\n",
    "\n",
    "# Encode the tokenized and padded input\n",
    "results = model(**x)\n",
    "print('output shape: ', results.last_hidden_state.shape)  # (batch_size x num_tokens x d_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrqTaJHVEF0r"
   },
   "source": [
    "### Tokenize the dataset\n",
    "We now tokenize the whole dataset using the batched map-method.\n",
    "\n",
    "TODO: Implement the tokenize_batch function using the tokenizer. Note: do not pad the data yet but truncate it to a max length of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MN1mFK1uEHeE"
   },
   "outputs": [],
   "source": [
    "def tokenize_batch(text_batch: List[str]):\n",
    "  # TODO: implement tokenize batch using the tokenizer\n",
    "  return tokenizer(text_batch, truncation=True, max_length=512, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "J9VfStgkEip8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2315 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/772 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after tokenization ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenize_batch to dataset (nothing TODO here)\n",
    "tokenized_dataset = dataset.map(lambda examples: tokenize_batch(examples[\"transcription\"]), batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['sample_name', 'transcription', 'description', 'keywords']).rename_column('medical_specialty', 'labels')\n",
    "print('Columns after tokenization', list(tokenized_dataset['train'].features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVq84WgQG9BR"
   },
   "source": [
    "### Token Pooling\n",
    "Define Pooling functions to compute sentence embeddings from token outputs.\n",
    "\n",
    "TODO: implement three possible pooling functions\n",
    "- CLS: use the output of the [CLS] token only and ignore the other tokens.\n",
    "- max/avg: globally max/avg pool over all token outputs, but make sure to ignore padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bQzwuFxMGax9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def CLS_pool(hidden_state: torch.FloatTensor, attention_mask: torch.BoolTensor):\n",
    "    \"\"\"\n",
    "    Returns only the hidden state of the [CLS] token\n",
    "    :param hidden_state: (N x M x d_hidden) where N is the batch size and M is the number of tokens\n",
    "    :param attention_mask: (N x M), True for \"real\" tokens, False for padding tokens.\n",
    "    :return (N x d_hidden)\n",
    "    \"\"\"\n",
    "    # TODO: implement function\n",
    "    return hidden_state[:, 0, :]\n",
    "\n",
    "def max_pool(hidden_state: torch.FloatTensor, attention_mask: torch.BoolTensor):\n",
    "    \"\"\"\n",
    "    Globally pools the hidden states over all tokens using max pooling.\n",
    "    :param hidden_state: (N x M x d_hidden) where N is the batch size and M is the number of tokens\n",
    "    :param attention_mask: (N x M), True for \"real\" tokens, False for padding tokens.\n",
    "    :return (N x d_hidden)\n",
    "    \"\"\"\n",
    "    # TODO: implement function\n",
    "    mask_expanded = attention_mask.bool().unsqueeze(-1).expand_as(hidden_state)\n",
    "    hidden_state_masked = torch.where(mask_expanded, hidden_state, torch.tensor(-1e9).to(hidden_state.dtype).to(hidden_state.device))\n",
    "\n",
    "    max_pooled, _ = torch.max(hidden_state_masked, dim=1)\n",
    "    return max_pooled\n",
    "\n",
    "def avg_pool(hidden_state: torch.FloatTensor, attention_mask: torch.BoolTensor):\n",
    "    \"\"\"\n",
    "    Globally pools the hidden states over all tokens using average pooling.\n",
    "    :param hidden_state: (N x M x d_hidden) where N is the batch size and M is the number of tokens\n",
    "    :param attention_mask: (N x M), True for \"real\" tokens, False for padding tokens.\n",
    "    :return (N x d_hidden)\n",
    "    \"\"\"\n",
    "    # TODO: implement function\n",
    "    mask_expanded = attention_mask.bool().unsqueeze(-1).expand_as(hidden_state).float()\n",
    "    sum_masks = mask_expanded.sum(1)  # (N x 1 x d_hidden)\n",
    "    \n",
    "    sum_pooled = torch.sum(hidden_state * mask_expanded, dim=1)\n",
    "    avg_pooled = sum_pooled / sum_masks.clamp(min=1e-9)\n",
    "\n",
    "    return avg_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7jf0k3_H5nQ"
   },
   "source": [
    "### Sentence Embedder\n",
    "The sentence embedder takes a tokenized input, uses the language model to compute token representations (i.e. the last hidden_state of the language model) and then uses a pooling function to compute a single sentence representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUZs8TqU2Gnl"
   },
   "source": [
    "TODO: implement the sentence embedder forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SLyFxgkTIA5h"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from typing import Dict\n",
    "\n",
    "class SentenceEmbedder(nn.Module):\n",
    "  def __init__(self, model, pool):\n",
    "    super().__init__()\n",
    "    self.model = model  # this is a huggingface language model from AutoModel.from_pretrained\n",
    "    self.pool = pool  # this is one of the three defined pooling functions (CLS, max, avg)\n",
    "\n",
    "  def forward(self, x: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :param x: dict containing the following elements:\n",
    "      - input_ids: torch.Tensor of the token_type ids with shape (N x M)\n",
    "      - attention_mask: torch.Tensor containing the attention mask of shape (N x M)\n",
    "    :return sentence_embedding for each sample of shape (N x d_hidden)\n",
    "    \"\"\"\n",
    "    # TODO: implement forward\n",
    "\n",
    "    outputs = self.model(input_ids=x['input_ids'], attention_mask=x['attention_mask'])\n",
    "    \n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    \n",
    "    sentence_embedding = self.pool(last_hidden_state, x['attention_mask'])\n",
    "\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAIiUT9D2Iu9"
   },
   "source": [
    "TODO: now decide which pooling function you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "w6J8KflrZFAj"
   },
   "outputs": [],
   "source": [
    "# TODO: try different pooling functions (nothing more to implement here)\n",
    "# (also change the dataset_name so you can later easily switch between pooling functions without the need to recompute the embeddings)\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    \n",
    "# device = 'cpu'\n",
    "\n",
    "dataset_name = 'encoded_transciptions_AVG'\n",
    "pool = avg_pool\n",
    "\n",
    "sentence_embedder = SentenceEmbedder(model, pool).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miiKNXPV2OS8"
   },
   "source": [
    " Now run the sentence embedder (nothing TODO here).\n",
    "\n",
    " The resulting dataset will then contain a sentence_embedding column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "M-q6nYkT0VkX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function embed_sentence at 0x16a136790> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2315 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/772 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2315 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/772 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def embed_sentence(batch):\n",
    "  model_input = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}\n",
    "  model_input = tokenizer.pad(model_input, return_tensors='pt').to(device=device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    sentence_embeddings = sentence_embedder(model_input)\n",
    "  return {'sentence_embedding': sentence_embeddings.detach().cpu().numpy()}\n",
    "\n",
    "encoded_dataset = tokenized_dataset.map(embed_sentence, batched=True, batch_size=64)\n",
    "encoded_dataset.save_to_disk(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QzqwS5rJCK9"
   },
   "source": [
    "## Step 5 - Train Classification Model on Sentence Embeddings\n",
    "Now we have a single sentence representation vector for each sentence. We now learn a simple classifier based on a MLP (i.e. a 2-layer fully-connected neural network). We first project each sentence embedding from d_hidden to d_mlp, apply ReLU (or another non-linearity) and then project the resulting vector into num_classes where we then apply the cross entropy loss. The classification head will then be learned based on the training data. This is the only learned component in our model.\n",
    "\n",
    "TODO: implement the MLP head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LNnx-9QzsIng"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import datasets\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "  def __init__(self, d_hidden: int, d_mlp: int, num_classes: int):\n",
    "    super().__init__()\n",
    "    # TODO: add the required layers here\n",
    "    self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(d_hidden, d_mlp),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_mlp, num_classes)\n",
    "    )\n",
    "\n",
    "    # TODO: define the loss function to use here\n",
    "    self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "  def forward(self, x, y_true):\n",
    "    \"\"\"\n",
    "    :param x: sentence embeddings (N x d_hidden)\n",
    "    :param y_true: target classes (multiclass) (N)\n",
    "    \"\"\"\n",
    "    # TODO: apply MLP head to sentence embeddings\n",
    "    # (N x num_classes)\n",
    "    logits = self.mlp_head(x)\n",
    "\n",
    "    # TODO: apply the loss function\n",
    "    loss = self.loss(logits, y_true) if y_true is not None else None\n",
    "    # TODO: compute the predictions (i.e. target classes as longs) from the logits\n",
    "    # (N)\n",
    "    _, y_pred = torch.max(logits, dim=1)\n",
    "\n",
    "    return y_pred, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTTplEVL3g_g"
   },
   "source": [
    "Now train the classification head on the sentence embeddings.\n",
    "The training is already implemented.\n",
    "\n",
    "TODO: specify and try different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZK_cuGPn1jUZ"
   },
   "outputs": [],
   "source": [
    "# TODO: try different hyperparameters\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "dataset_name = 'encoded_transciptions_AVG'\n",
    "num_epochs = 20\n",
    "lr = 5e-4\n",
    "weight_decay = 1e-4\n",
    "d_hidden = 768  # must match the language model hidden output\n",
    "d_mlp = 1024\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "5MXXBwIyqAjU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classes:  12\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 19/19 [00:00<00:00, 38.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  2.1185073852539062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 195.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.7768936157226562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 201.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.593662977218628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 173.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.5410226583480835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 177.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.4411345720291138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 175.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.4048923254013062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 172.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.3656760454177856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 178.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.3311318159103394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 174.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.2859617471694946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 171.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.2920438051223755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 146.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.2504011392593384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 147.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.2291159629821777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 145.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.19967520236969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 145.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.19120454788208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 144.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.1519535779953003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 130.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.1699175834655762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 121.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.1350510120391846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 113.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.145141363143921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 108.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.1350880861282349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 19/19 [00:00<00:00, 103.13it/s]\n",
      "/var/folders/lb/3yybq3q15h95_b40m72w81vc0000gn/T/ipykernel_56845/1963609609.py:39: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  f1_metric = datasets.load_metric(\"f1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1.0997015237808228\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 7/7 [00:00<00:00, 106.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test loss:  1.28374183177948\n",
      "F1 (Macro):  {'f1': 0.3769100747896881}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now run the training...\n",
    "\n",
    "print('num classes: ', len(class_names))\n",
    "encoded_dataset = load_from_disk(dataset_name)\n",
    "# make sure PyTorch Tensors are returned\n",
    "encoded_dataset.set_format('pt')\n",
    "\n",
    "# remove all columns except sentence_embedding and labels\n",
    "columns_to_remove = set(encoded_dataset['train'].column_names) - {'sentence_embedding', 'labels'}\n",
    "encoded_dataset = encoded_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "train_data_loader = DataLoader(encoded_dataset['train'], batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(encoded_dataset['test'], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classification_model = ClassificationHead(d_hidden=d_hidden,\n",
    "                                          d_mlp=d_mlp,\n",
    "                                          num_classes=len(class_names))\n",
    "classification_model = classification_model.to(device=device)\n",
    "optimizer = torch.optim.Adam(classification_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "classification_model.train()\n",
    "print('Training...')\n",
    "for epoch in range(num_epochs):\n",
    "  train_loss = []\n",
    "  for train_batch in tqdm(train_data_loader):\n",
    "    x = train_batch['sentence_embedding'].to(device=device)\n",
    "    y_true = train_batch['labels'].to(device=device)\n",
    "\n",
    "    y_pred, loss = classification_model(x, y_true)\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "  print('train loss: ', torch.mean(torch.tensor(train_loss)).item())\n",
    "\n",
    "classification_model.eval()\n",
    "print('Testing...')\n",
    "f1_metric = datasets.load_metric(\"f1\")\n",
    "with torch.no_grad():\n",
    "  test_loss = []\n",
    "  for test_batch in tqdm(test_data_loader):\n",
    "    x = test_batch['sentence_embedding'].to(device=device)\n",
    "    y_true = test_batch['labels'].to(device=device)\n",
    "\n",
    "    y_pred, loss = classification_model(x, y_true)\n",
    "\n",
    "    f1_metric.add_batch(predictions=y_pred, references=y_true)\n",
    "    test_loss.append(loss)\n",
    "print('\\ntest loss: ', torch.mean(torch.tensor(test_loss)).item())\n",
    "print('F1 (Macro): ', f1_metric.compute(average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyGeE0JKWmJN"
   },
   "source": [
    "# Task 3: Generating text from a pre-trained decoder LM (graded!)\n",
    "## Goals\n",
    "- Understand how inference (text generation) works on decoder languages models\n",
    "\n",
    "## Tools\n",
    "- [Huggingface Transformers](https://huggingface.co/transformers/): Library for pre-trained transformer-based language models)\n",
    "\n",
    "## Model\n",
    "- Architecture: https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel\n",
    "- Model weights: https://huggingface.co/healx/gpt-2-pubmed-medium\n",
    "\n",
    "## Notes and Tips\n",
    "- You can read this blog post to learn more about generation strategies: https://huggingface.co/blog/how-to-generate\n",
    "- We are going to implement greedy-search.\n",
    "- Please do not use the \"generate\" method of the pre-trained model (as there generation is already implemented)\n",
    "- You can however take a look at their implementation of greedy-search: https://github.com/huggingface/transformers/blob/04ab5605fbb4ef207b10bf2772d88c53fc242e83/src/transformers/generation/utils.py#L2080\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlHUnMYEY49m"
   },
   "source": [
    "## Step 1: Implement generate method\n",
    "Implement the generate method that takes a language model and its tokenizer together with a list of text prefixes and outputs a list of generated sentences (one for each prefix). The prefixes should be \"autocompleted\" by the model.\n",
    "Use the greedy-search method and process all prefixes as a single batch.\n",
    "\n",
    "TODO: finish the implementation of the generate method by filling in the missing lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTkJ3hZqZsAK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "\n",
    "def generate(model, tokenizer, prefix: List[str], max_predicted: int) -> List[str]:\n",
    "  \"\"\"\n",
    "  :param model: PreTrainedModel.\n",
    "  :param tokenizer: PreTrainedTokenizer (https://huggingface.co/docs/transformers/main/main_classes/tokenizer#transformers.PreTrainedTokenizer)\n",
    "  :prefix: Batch of prefixes to be autocompleted by the language model\n",
    "  \"\"\"\n",
    "  if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "  pad_token_id = tokenizer.pad_token_id\n",
    "  bos_token_id = tokenizer.bos_token_id\n",
    "  eos_token_id = tokenizer.eos_token_id\n",
    "  device = model.device\n",
    "\n",
    "  # 1. Tokenize the prefixes and prepare them for input into the language model\n",
    "  #    Notes\n",
    "  #    - add the start token (BOS) but not the end token (EOS)\n",
    "  #    - left! pad them to the maximum length in the batch, i.e. paddding is left to the \"real\" tokens\n",
    "  #    - inputs_ids and attention_mask should be computes, and should already be stacked in the batch dim\n",
    "  #    -> this can all be done by calling the tokenizer! See: https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__\n",
    "  tokenizer.padding_side = 'left'\n",
    "  # TODO: call the tokenizer to prepare the inputs (a dictionary)\n",
    "  inputs = tokenizer(...  # add arguments here\n",
    "                     return_tensors='pt')\n",
    "  inputs = inputs.to(device=device)\n",
    "  input_ids = inputs['input_ids']\n",
    "  attention_mask = inputs['attention_mask']\n",
    "\n",
    "  # TODO: Compute position ids based on the attention mask\n",
    "  # Note: the position ids are the indices of the positions, starting with zero and increasing for each non-padding token\n",
    "  # The positions ids for padding tokens can take any value\n",
    "  position_ids: torch.LongTensor = None\n",
    "\n",
    "  # Initialize some variables (nothing TODO here)\n",
    "  N = position_ids.shape[0]\n",
    "  past_key_values = None\n",
    "  unfinished_sequences = torch.ones(N, dtype=torch.bool, device=device)\n",
    "  predicted_input_ids = input_ids.clone()\n",
    "\n",
    "  while True:\n",
    "    # 2. Predict the next token logits by passing the previous inputs into the language model\n",
    "    #    Note: already computed steps can be given by past_key_values, other steps are given as input_ids\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        return_dict=True\n",
    "    )\n",
    "    # (N x |V|)\n",
    "    next_token_logits: torch.LongTensor = outputs.logits[:, -1, :]\n",
    "\n",
    "    # 3. Select the next predicted tokens by greedy search\n",
    "    # (N)\n",
    "    next_tokens: torch.LongTensor = None  # TODO\n",
    "\n",
    "    # 4. For all already finished sentences, replace the next_tokens by <PAD>\n",
    "    next_tokens = None  # TODO\n",
    "\n",
    "    # 5. Check which sentences are already finished by checking whether they contain <END>\n",
    "    # Which sentences have been finished with the predicted token (next_tokens)\n",
    "    # (N)\n",
    "    newly_finished = None  # TODO\n",
    "    # Which sentences, therefore, remain unfinished\n",
    "    # (N)\n",
    "    unfinished_sequences = None  # TODO\n",
    "\n",
    "    # 6. Concatenate the next input to predicted_input_ids, attention_mask\n",
    "    # (N x M_new) where M_new is one longer than the previous M of predicted_input_ids\n",
    "    predicted_input_ids = None  # TODO\n",
    "    # (N x M_new)\n",
    "    attention_mask = None  # TODO: extend attention mask by ones for next step\n",
    "    # Nothing TODO here\n",
    "    position_ids = position_ids.amax(dim=1, keepdim=True) + 1\n",
    "    input_ids = next_tokens.unsqueeze(dim=1)\n",
    "    past_key_values = outputs.past_key_values\n",
    "\n",
    "    # 7. Check if finished (nothing TODO here)\n",
    "    if predicted_input_ids.shape[1] >= max_predicted or unfinished_sequences.max() == 0:\n",
    "      break\n",
    "\n",
    "  # 8. Convert back to text (nothing TODO here)\n",
    "  generated_sentences: List[str] = tokenizer.batch_decode(predicted_input_ids, skip_special_tokens=True)\n",
    "  return generated_sentences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQzhPO9Bx6bx"
   },
   "source": [
    "## Step 2: Load a decoder language model (nothing TODO here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSObYXZCZ77g"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "decoder_model_name = \"gpt2-medium\"\n",
    "\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(decoder_model_name)\n",
    "decoder_model = AutoModelForCausalLM.from_pretrained(decoder_model_name)\n",
    "decoder_model = decoder_model.to(device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3zq42ZlyQpU"
   },
   "source": [
    "## Step 3: Apply the generate method to some example text (nothing TODO here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN8GuYvhyYdk"
   },
   "outputs": [],
   "source": [
    "# You can play around with different texts here\n",
    "prefix_1 = \"This sentences is about\"\n",
    "prefix_2 = \"Can you complete this sentence?\"\n",
    "prefixes = [prefix_1, prefix_2]\n",
    "\n",
    "generated_sentences = generate(decoder_model, decoder_tokenizer, prefixes, max_predicted=50)\n",
    "\n",
    "for i, (prefix, sent) in enumerate(zip(prefixes, generated_sentences)):\n",
    "  print(f'Completed sentence {i}: \"{prefix}\"')\n",
    "  print(\"-------------------------------------------------------\\n\")\n",
    "  print(sent)\n",
    "  print(\"\\n=======================================================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_M43bZ4s9UCH"
   },
   "source": [
    "As you can see, the results tend to be quite repetetive.\n",
    "This is why greedy search is typically not used in practice.\n",
    "Common alternatives include beam search and sampling from the distribution of next tokens. While this is not part of the mandatory exercise, you can try to implement other generation methods as part of a bonus task and see how it can improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWu4pDaJy2XG"
   },
   "source": [
    "## Extra Task (not graded): Implement a beam search generate function and apply it\n",
    "Re-implement the generate method but instead of doing greedy search, use beam search instead.\n",
    "\n",
    "Note that this is a completely optional and non-graded task and will not be discussed in the soutions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zewLo1VLzfpg"
   },
   "outputs": [],
   "source": [
    "def beam_search_generate(model, tokenizer, prefix: List[str], max_predicted: int, num_beams: int = 5) -> List[str]:\n",
    "  pass  # TODO"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "dz_Is5Bw_PZC",
    "I62jANWNvkzp",
    "fBPyy6E5wpVC"
   ],
   "provenance": [
    {
     "file_id": "1g8x2r2zKvDgL_YO7WsixKwCq80xoFod3",
     "timestamp": 1715611942175
    },
    {
     "file_id": "1YVhdEh9KDSoaPWyq8WU0b4tKr5XfTkKp",
     "timestamp": 1683023138687
    },
    {
     "file_id": "1I8W6SJAHopCX3Ziq5DNZJxpYTImENO7r",
     "timestamp": 1682088432737
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
